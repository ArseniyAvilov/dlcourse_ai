{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.50940412e-05 6.69254912e-03 9.93262357e-01]\n",
      "d =  5.006760443547122\n"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "print(probs)\n",
    "d = linear_classifer.cross_entropy_loss(probs, 1)\n",
    "print(\"d = \", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302317\n",
      "Epoch 1, loss: 2.303185\n",
      "Epoch 2, loss: 2.302271\n",
      "Epoch 3, loss: 2.303194\n",
      "Epoch 4, loss: 2.302338\n",
      "Epoch 5, loss: 2.301946\n",
      "Epoch 6, loss: 2.302544\n",
      "Epoch 7, loss: 2.301348\n",
      "Epoch 8, loss: 2.301076\n",
      "Epoch 9, loss: 2.301775\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7de08f6c18>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.113\n",
      "Epoch 0, loss: 2.303240\n",
      "Epoch 1, loss: 2.302430\n",
      "Epoch 2, loss: 2.300641\n",
      "Epoch 3, loss: 2.302155\n",
      "Epoch 4, loss: 2.302494\n",
      "Epoch 5, loss: 2.300200\n",
      "Epoch 6, loss: 2.300596\n",
      "Epoch 7, loss: 2.301555\n",
      "Epoch 8, loss: 2.301820\n",
      "Epoch 9, loss: 2.301789\n",
      "Epoch 10, loss: 2.300205\n",
      "Epoch 11, loss: 2.301226\n",
      "Epoch 12, loss: 2.300057\n",
      "Epoch 13, loss: 2.299913\n",
      "Epoch 14, loss: 2.301913\n",
      "Epoch 15, loss: 2.301299\n",
      "Epoch 16, loss: 2.301105\n",
      "Epoch 17, loss: 2.301654\n",
      "Epoch 18, loss: 2.300629\n",
      "Epoch 19, loss: 2.302569\n",
      "Epoch 20, loss: 2.300418\n",
      "Epoch 21, loss: 2.302342\n",
      "Epoch 22, loss: 2.301812\n",
      "Epoch 23, loss: 2.302024\n",
      "Epoch 24, loss: 2.302636\n",
      "Epoch 25, loss: 2.302546\n",
      "Epoch 26, loss: 2.301827\n",
      "Epoch 27, loss: 2.302267\n",
      "Epoch 28, loss: 2.302103\n",
      "Epoch 29, loss: 2.301126\n",
      "Epoch 30, loss: 2.302257\n",
      "Epoch 31, loss: 2.301800\n",
      "Epoch 32, loss: 2.301003\n",
      "Epoch 33, loss: 2.301999\n",
      "Epoch 34, loss: 2.301478\n",
      "Epoch 35, loss: 2.301324\n",
      "Epoch 36, loss: 2.302488\n",
      "Epoch 37, loss: 2.301382\n",
      "Epoch 38, loss: 2.300878\n",
      "Epoch 39, loss: 2.301532\n",
      "Epoch 40, loss: 2.300834\n",
      "Epoch 41, loss: 2.301374\n",
      "Epoch 42, loss: 2.301268\n",
      "Epoch 43, loss: 2.300711\n",
      "Epoch 44, loss: 2.300917\n",
      "Epoch 45, loss: 2.301432\n",
      "Epoch 46, loss: 2.301250\n",
      "Epoch 47, loss: 2.301330\n",
      "Epoch 48, loss: 2.301697\n",
      "Epoch 49, loss: 2.301041\n",
      "Epoch 50, loss: 2.300912\n",
      "Epoch 51, loss: 2.300558\n",
      "Epoch 52, loss: 2.301400\n",
      "Epoch 53, loss: 2.300047\n",
      "Epoch 54, loss: 2.300803\n",
      "Epoch 55, loss: 2.300817\n",
      "Epoch 56, loss: 2.301299\n",
      "Epoch 57, loss: 2.301152\n",
      "Epoch 58, loss: 2.301192\n",
      "Epoch 59, loss: 2.301935\n",
      "Epoch 60, loss: 2.300793\n",
      "Epoch 61, loss: 2.300808\n",
      "Epoch 62, loss: 2.301492\n",
      "Epoch 63, loss: 2.300544\n",
      "Epoch 64, loss: 2.300744\n",
      "Epoch 65, loss: 2.300692\n",
      "Epoch 66, loss: 2.301525\n",
      "Epoch 67, loss: 2.301441\n",
      "Epoch 68, loss: 2.300838\n",
      "Epoch 69, loss: 2.301331\n",
      "Epoch 70, loss: 2.301637\n",
      "Epoch 71, loss: 2.300510\n",
      "Epoch 72, loss: 2.300817\n",
      "Epoch 73, loss: 2.300987\n",
      "Epoch 74, loss: 2.301678\n",
      "Epoch 75, loss: 2.300731\n",
      "Epoch 76, loss: 2.300664\n",
      "Epoch 77, loss: 2.301698\n",
      "Epoch 78, loss: 2.300359\n",
      "Epoch 79, loss: 2.300616\n",
      "Epoch 80, loss: 2.300331\n",
      "Epoch 81, loss: 2.300961\n",
      "Epoch 82, loss: 2.300702\n",
      "Epoch 83, loss: 2.299731\n",
      "Epoch 84, loss: 2.301319\n",
      "Epoch 85, loss: 2.301088\n",
      "Epoch 86, loss: 2.300128\n",
      "Epoch 87, loss: 2.301373\n",
      "Epoch 88, loss: 2.300204\n",
      "Epoch 89, loss: 2.301074\n",
      "Epoch 90, loss: 2.301491\n",
      "Epoch 91, loss: 2.300752\n",
      "Epoch 92, loss: 2.301953\n",
      "Epoch 93, loss: 2.301094\n",
      "Epoch 94, loss: 2.301463\n",
      "Epoch 95, loss: 2.300271\n",
      "Epoch 96, loss: 2.300402\n",
      "Epoch 97, loss: 2.299961\n",
      "Epoch 98, loss: 2.300774\n",
      "Epoch 99, loss: 2.301194\n",
      "Accuracy after training for 100 epochs:  0.116\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302910\n",
      "Epoch 1, loss: 2.302262\n",
      "Epoch 2, loss: 2.302231\n",
      "Epoch 3, loss: 2.301475\n",
      "Epoch 4, loss: 2.302454\n",
      "Epoch 5, loss: 2.301449\n",
      "Epoch 6, loss: 2.302880\n",
      "Epoch 7, loss: 2.302612\n",
      "Epoch 8, loss: 2.303635\n",
      "Epoch 9, loss: 2.302136\n",
      "Epoch 10, loss: 2.302758\n",
      "Epoch 11, loss: 2.302651\n",
      "Epoch 12, loss: 2.303074\n",
      "Epoch 13, loss: 2.302155\n",
      "Epoch 14, loss: 2.301867\n",
      "Epoch 15, loss: 2.301608\n",
      "Epoch 16, loss: 2.302311\n",
      "Epoch 17, loss: 2.301678\n",
      "Epoch 18, loss: 2.302088\n",
      "Epoch 19, loss: 2.300768\n",
      "Epoch 20, loss: 2.302106\n",
      "Epoch 21, loss: 2.301208\n",
      "Epoch 22, loss: 2.301248\n",
      "Epoch 23, loss: 2.302740\n",
      "Epoch 24, loss: 2.302275\n",
      "Epoch 25, loss: 2.301305\n",
      "Epoch 26, loss: 2.301006\n",
      "Epoch 27, loss: 2.301043\n",
      "Epoch 28, loss: 2.301595\n",
      "Epoch 29, loss: 2.301170\n",
      "Epoch 30, loss: 2.302663\n",
      "Epoch 31, loss: 2.302173\n",
      "Epoch 32, loss: 2.300358\n",
      "Epoch 33, loss: 2.300666\n",
      "Epoch 34, loss: 2.301258\n",
      "Epoch 35, loss: 2.300991\n",
      "Epoch 36, loss: 2.301133\n",
      "Epoch 37, loss: 2.301241\n",
      "Epoch 38, loss: 2.299848\n",
      "Epoch 39, loss: 2.302330\n",
      "Epoch 40, loss: 2.300757\n",
      "Epoch 41, loss: 2.301951\n",
      "Epoch 42, loss: 2.300671\n",
      "Epoch 43, loss: 2.302451\n",
      "Epoch 44, loss: 2.300016\n",
      "Epoch 45, loss: 2.300538\n",
      "Epoch 46, loss: 2.300719\n",
      "Epoch 47, loss: 2.300967\n",
      "Epoch 48, loss: 2.301383\n",
      "Epoch 49, loss: 2.299075\n",
      "Epoch 50, loss: 2.298739\n",
      "Epoch 51, loss: 2.300648\n",
      "Epoch 52, loss: 2.300057\n",
      "Epoch 53, loss: 2.299301\n",
      "Epoch 54, loss: 2.300364\n",
      "Epoch 55, loss: 2.300810\n",
      "Epoch 56, loss: 2.300468\n",
      "Epoch 57, loss: 2.302202\n",
      "Epoch 58, loss: 2.300839\n",
      "Epoch 59, loss: 2.300853\n",
      "Epoch 60, loss: 2.299239\n",
      "Epoch 61, loss: 2.300045\n",
      "Epoch 62, loss: 2.300157\n",
      "Epoch 63, loss: 2.299710\n",
      "Epoch 64, loss: 2.300462\n",
      "Epoch 65, loss: 2.302784\n",
      "Epoch 66, loss: 2.300263\n",
      "Epoch 67, loss: 2.300122\n",
      "Epoch 68, loss: 2.300935\n",
      "Epoch 69, loss: 2.299480\n",
      "Epoch 70, loss: 2.300464\n",
      "Epoch 71, loss: 2.298914\n",
      "Epoch 72, loss: 2.299340\n",
      "Epoch 73, loss: 2.301180\n",
      "Epoch 74, loss: 2.299295\n",
      "Epoch 75, loss: 2.300617\n",
      "Epoch 76, loss: 2.300594\n",
      "Epoch 77, loss: 2.300380\n",
      "Epoch 78, loss: 2.301311\n",
      "Epoch 79, loss: 2.298581\n",
      "Epoch 80, loss: 2.300905\n",
      "Epoch 81, loss: 2.299762\n",
      "Epoch 82, loss: 2.297984\n",
      "Epoch 83, loss: 2.298564\n",
      "Epoch 84, loss: 2.298121\n",
      "Epoch 85, loss: 2.298672\n",
      "Epoch 86, loss: 2.300825\n",
      "Epoch 87, loss: 2.298040\n",
      "Epoch 88, loss: 2.301227\n",
      "Epoch 89, loss: 2.300668\n",
      "Epoch 90, loss: 2.299515\n",
      "Epoch 91, loss: 2.299975\n",
      "Epoch 92, loss: 2.299834\n",
      "Epoch 93, loss: 2.298503\n",
      "Epoch 94, loss: 2.298262\n",
      "Epoch 95, loss: 2.299045\n",
      "Epoch 96, loss: 2.301965\n",
      "Epoch 97, loss: 2.300772\n",
      "Epoch 98, loss: 2.299521\n",
      "Epoch 99, loss: 2.298876\n",
      "Epoch 100, loss: 2.299106\n",
      "Epoch 101, loss: 2.298901\n",
      "Epoch 102, loss: 2.298036\n",
      "Epoch 103, loss: 2.299613\n",
      "Epoch 104, loss: 2.298739\n",
      "Epoch 105, loss: 2.300339\n",
      "Epoch 106, loss: 2.298515\n",
      "Epoch 107, loss: 2.298683\n",
      "Epoch 108, loss: 2.299856\n",
      "Epoch 109, loss: 2.299057\n",
      "Epoch 110, loss: 2.298905\n",
      "Epoch 111, loss: 2.299689\n",
      "Epoch 112, loss: 2.300062\n",
      "Epoch 113, loss: 2.299146\n",
      "Epoch 114, loss: 2.297651\n",
      "Epoch 115, loss: 2.298696\n",
      "Epoch 116, loss: 2.298226\n",
      "Epoch 117, loss: 2.300382\n",
      "Epoch 118, loss: 2.297540\n",
      "Epoch 119, loss: 2.298721\n",
      "Epoch 120, loss: 2.299536\n",
      "Epoch 121, loss: 2.295389\n",
      "Epoch 122, loss: 2.296156\n",
      "Epoch 123, loss: 2.298140\n",
      "Epoch 124, loss: 2.299730\n",
      "Epoch 125, loss: 2.298792\n",
      "Epoch 126, loss: 2.297462\n",
      "Epoch 127, loss: 2.299276\n",
      "Epoch 128, loss: 2.297255\n",
      "Epoch 129, loss: 2.298915\n",
      "Epoch 130, loss: 2.299438\n",
      "Epoch 131, loss: 2.298235\n",
      "Epoch 132, loss: 2.295186\n",
      "Epoch 133, loss: 2.296986\n",
      "Epoch 134, loss: 2.297977\n",
      "Epoch 135, loss: 2.297103\n",
      "Epoch 136, loss: 2.295046\n",
      "Epoch 137, loss: 2.295007\n",
      "Epoch 138, loss: 2.297915\n",
      "Epoch 139, loss: 2.297309\n",
      "Epoch 140, loss: 2.297929\n",
      "Epoch 141, loss: 2.299797\n",
      "Epoch 142, loss: 2.297219\n",
      "Epoch 143, loss: 2.296715\n",
      "Epoch 144, loss: 2.298877\n",
      "Epoch 145, loss: 2.297119\n",
      "Epoch 146, loss: 2.297902\n",
      "Epoch 147, loss: 2.298971\n",
      "Epoch 148, loss: 2.296413\n",
      "Epoch 149, loss: 2.295677\n",
      "Epoch 150, loss: 2.297022\n",
      "Epoch 151, loss: 2.298379\n",
      "Epoch 152, loss: 2.298437\n",
      "Epoch 153, loss: 2.298539\n",
      "Epoch 154, loss: 2.295900\n",
      "Epoch 155, loss: 2.300278\n",
      "Epoch 156, loss: 2.297063\n",
      "Epoch 157, loss: 2.296391\n",
      "Epoch 158, loss: 2.297867\n",
      "Epoch 159, loss: 2.298239\n",
      "Epoch 160, loss: 2.299761\n",
      "Epoch 161, loss: 2.294192\n",
      "Epoch 162, loss: 2.297581\n",
      "Epoch 163, loss: 2.296489\n",
      "Epoch 164, loss: 2.299382\n",
      "Epoch 165, loss: 2.296834\n",
      "Epoch 166, loss: 2.294777\n",
      "Epoch 167, loss: 2.297896\n",
      "Epoch 168, loss: 2.297657\n",
      "Epoch 169, loss: 2.294648\n",
      "Epoch 170, loss: 2.296270\n",
      "Epoch 171, loss: 2.295553\n",
      "Epoch 172, loss: 2.295553\n",
      "Epoch 173, loss: 2.297266\n",
      "Epoch 174, loss: 2.295588\n",
      "Epoch 175, loss: 2.294424\n",
      "Epoch 176, loss: 2.297614\n",
      "Epoch 177, loss: 2.296821\n",
      "Epoch 178, loss: 2.296390\n",
      "Epoch 179, loss: 2.295079\n",
      "Epoch 180, loss: 2.294550\n",
      "Epoch 181, loss: 2.294986\n",
      "Epoch 182, loss: 2.297696\n",
      "Epoch 183, loss: 2.298023\n",
      "Epoch 184, loss: 2.295001\n",
      "Epoch 185, loss: 2.296646\n",
      "Epoch 186, loss: 2.297096\n",
      "Epoch 187, loss: 2.293957\n",
      "Epoch 188, loss: 2.301215\n",
      "Epoch 189, loss: 2.297544\n",
      "Epoch 190, loss: 2.294331\n",
      "Epoch 191, loss: 2.293206\n",
      "Epoch 192, loss: 2.295360\n",
      "Epoch 193, loss: 2.293434\n",
      "Epoch 194, loss: 2.296822\n",
      "Epoch 195, loss: 2.296401\n",
      "Epoch 196, loss: 2.295318\n",
      "Epoch 197, loss: 2.296080\n",
      "Epoch 198, loss: 2.296466\n",
      "Epoch 199, loss: 2.297285\n",
      "Epoch 0, loss: 2.301991\n",
      "Epoch 1, loss: 2.303054\n",
      "Epoch 2, loss: 2.301190\n",
      "Epoch 3, loss: 2.302524\n",
      "Epoch 4, loss: 2.301920\n",
      "Epoch 5, loss: 2.302028\n",
      "Epoch 6, loss: 2.301920\n",
      "Epoch 7, loss: 2.301766\n",
      "Epoch 8, loss: 2.302110\n",
      "Epoch 9, loss: 2.302967\n",
      "Epoch 10, loss: 2.301673\n",
      "Epoch 11, loss: 2.302563\n",
      "Epoch 12, loss: 2.302881\n",
      "Epoch 13, loss: 2.301241\n",
      "Epoch 14, loss: 2.301947\n",
      "Epoch 15, loss: 2.301648\n",
      "Epoch 16, loss: 2.303215\n",
      "Epoch 17, loss: 2.301443\n",
      "Epoch 18, loss: 2.301686\n",
      "Epoch 19, loss: 2.301669\n",
      "Epoch 20, loss: 2.302523\n",
      "Epoch 21, loss: 2.301458\n",
      "Epoch 22, loss: 2.301245\n",
      "Epoch 23, loss: 2.300722\n",
      "Epoch 24, loss: 2.301777\n",
      "Epoch 25, loss: 2.301805\n",
      "Epoch 26, loss: 2.300890\n",
      "Epoch 27, loss: 2.302159\n",
      "Epoch 28, loss: 2.302392\n",
      "Epoch 29, loss: 2.301436\n",
      "Epoch 30, loss: 2.301884\n",
      "Epoch 31, loss: 2.301770\n",
      "Epoch 32, loss: 2.302818\n",
      "Epoch 33, loss: 2.302041\n",
      "Epoch 34, loss: 2.301466\n",
      "Epoch 35, loss: 2.302675\n",
      "Epoch 36, loss: 2.301361\n",
      "Epoch 37, loss: 2.301161\n",
      "Epoch 38, loss: 2.301964\n",
      "Epoch 39, loss: 2.299661\n",
      "Epoch 40, loss: 2.300541\n",
      "Epoch 41, loss: 2.302767\n",
      "Epoch 42, loss: 2.300959\n",
      "Epoch 43, loss: 2.300383\n",
      "Epoch 44, loss: 2.301238\n",
      "Epoch 45, loss: 2.298558\n",
      "Epoch 46, loss: 2.301302\n",
      "Epoch 47, loss: 2.301395\n",
      "Epoch 48, loss: 2.301439\n",
      "Epoch 49, loss: 2.301826\n",
      "Epoch 50, loss: 2.300402\n",
      "Epoch 51, loss: 2.301245\n",
      "Epoch 52, loss: 2.300620\n",
      "Epoch 53, loss: 2.302662\n",
      "Epoch 54, loss: 2.299491\n",
      "Epoch 55, loss: 2.301282\n",
      "Epoch 56, loss: 2.300895\n",
      "Epoch 57, loss: 2.300051\n",
      "Epoch 58, loss: 2.300671\n",
      "Epoch 59, loss: 2.299956\n",
      "Epoch 60, loss: 2.300002\n",
      "Epoch 61, loss: 2.301054\n",
      "Epoch 62, loss: 2.299976\n",
      "Epoch 63, loss: 2.300396\n",
      "Epoch 64, loss: 2.300256\n",
      "Epoch 65, loss: 2.299801\n",
      "Epoch 66, loss: 2.300999\n",
      "Epoch 67, loss: 2.301156\n",
      "Epoch 68, loss: 2.298050\n",
      "Epoch 69, loss: 2.300303\n",
      "Epoch 70, loss: 2.300271\n",
      "Epoch 71, loss: 2.300260\n",
      "Epoch 72, loss: 2.299268\n",
      "Epoch 73, loss: 2.301537\n",
      "Epoch 74, loss: 2.298150\n",
      "Epoch 75, loss: 2.302967\n",
      "Epoch 76, loss: 2.300956\n",
      "Epoch 77, loss: 2.298919\n",
      "Epoch 78, loss: 2.299892\n",
      "Epoch 79, loss: 2.300555\n",
      "Epoch 80, loss: 2.300697\n",
      "Epoch 81, loss: 2.299125\n",
      "Epoch 82, loss: 2.299110\n",
      "Epoch 83, loss: 2.299458\n",
      "Epoch 84, loss: 2.298838\n",
      "Epoch 85, loss: 2.301183\n",
      "Epoch 86, loss: 2.300129\n",
      "Epoch 87, loss: 2.298536\n",
      "Epoch 88, loss: 2.300769\n",
      "Epoch 89, loss: 2.299512\n",
      "Epoch 90, loss: 2.299468\n",
      "Epoch 91, loss: 2.297163\n",
      "Epoch 92, loss: 2.300709\n",
      "Epoch 93, loss: 2.299765\n",
      "Epoch 94, loss: 2.300875\n",
      "Epoch 95, loss: 2.299307\n",
      "Epoch 96, loss: 2.298359\n",
      "Epoch 97, loss: 2.299623\n",
      "Epoch 98, loss: 2.299531\n",
      "Epoch 99, loss: 2.297297\n",
      "Epoch 100, loss: 2.300404\n",
      "Epoch 101, loss: 2.300051\n",
      "Epoch 102, loss: 2.297291\n",
      "Epoch 103, loss: 2.299301\n",
      "Epoch 104, loss: 2.301691\n",
      "Epoch 105, loss: 2.301119\n",
      "Epoch 106, loss: 2.298325\n",
      "Epoch 107, loss: 2.299333\n",
      "Epoch 108, loss: 2.298729\n",
      "Epoch 109, loss: 2.297865\n",
      "Epoch 110, loss: 2.298314\n",
      "Epoch 111, loss: 2.299538\n",
      "Epoch 112, loss: 2.300021\n",
      "Epoch 113, loss: 2.298400\n",
      "Epoch 114, loss: 2.298249\n",
      "Epoch 115, loss: 2.299136\n",
      "Epoch 116, loss: 2.302077\n",
      "Epoch 117, loss: 2.300473\n",
      "Epoch 118, loss: 2.297262\n",
      "Epoch 119, loss: 2.297125\n",
      "Epoch 120, loss: 2.298217\n",
      "Epoch 121, loss: 2.297883\n",
      "Epoch 122, loss: 2.299048\n",
      "Epoch 123, loss: 2.299807\n",
      "Epoch 124, loss: 2.299048\n",
      "Epoch 125, loss: 2.296568\n",
      "Epoch 126, loss: 2.297938\n",
      "Epoch 127, loss: 2.297094\n",
      "Epoch 128, loss: 2.299827\n",
      "Epoch 129, loss: 2.299811\n",
      "Epoch 130, loss: 2.298792\n",
      "Epoch 131, loss: 2.296130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132, loss: 2.298911\n",
      "Epoch 133, loss: 2.299317\n",
      "Epoch 134, loss: 2.298417\n",
      "Epoch 135, loss: 2.300275\n",
      "Epoch 136, loss: 2.297515\n",
      "Epoch 137, loss: 2.300116\n",
      "Epoch 138, loss: 2.296555\n",
      "Epoch 139, loss: 2.299465\n",
      "Epoch 140, loss: 2.298293\n",
      "Epoch 141, loss: 2.297275\n",
      "Epoch 142, loss: 2.295382\n",
      "Epoch 143, loss: 2.297736\n",
      "Epoch 144, loss: 2.298156\n",
      "Epoch 145, loss: 2.295168\n",
      "Epoch 146, loss: 2.298762\n",
      "Epoch 147, loss: 2.300393\n",
      "Epoch 148, loss: 2.297504\n",
      "Epoch 149, loss: 2.298302\n",
      "Epoch 150, loss: 2.296923\n",
      "Epoch 151, loss: 2.298362\n",
      "Epoch 152, loss: 2.298319\n",
      "Epoch 153, loss: 2.295949\n",
      "Epoch 154, loss: 2.297847\n",
      "Epoch 155, loss: 2.298746\n",
      "Epoch 156, loss: 2.296151\n",
      "Epoch 157, loss: 2.294977\n",
      "Epoch 158, loss: 2.296167\n",
      "Epoch 159, loss: 2.299008\n",
      "Epoch 160, loss: 2.296435\n",
      "Epoch 161, loss: 2.299412\n",
      "Epoch 162, loss: 2.294999\n",
      "Epoch 163, loss: 2.296405\n",
      "Epoch 164, loss: 2.297677\n",
      "Epoch 165, loss: 2.297589\n",
      "Epoch 166, loss: 2.295697\n",
      "Epoch 167, loss: 2.296061\n",
      "Epoch 168, loss: 2.297162\n",
      "Epoch 169, loss: 2.295784\n",
      "Epoch 170, loss: 2.296876\n",
      "Epoch 171, loss: 2.295855\n",
      "Epoch 172, loss: 2.294251\n",
      "Epoch 173, loss: 2.299078\n",
      "Epoch 174, loss: 2.295918\n",
      "Epoch 175, loss: 2.297561\n",
      "Epoch 176, loss: 2.295543\n",
      "Epoch 177, loss: 2.297171\n",
      "Epoch 178, loss: 2.297053\n",
      "Epoch 179, loss: 2.296013\n",
      "Epoch 180, loss: 2.299039\n",
      "Epoch 181, loss: 2.298433\n",
      "Epoch 182, loss: 2.294149\n",
      "Epoch 183, loss: 2.297683\n",
      "Epoch 184, loss: 2.294028\n",
      "Epoch 185, loss: 2.295963\n",
      "Epoch 186, loss: 2.295567\n",
      "Epoch 187, loss: 2.296702\n",
      "Epoch 188, loss: 2.295564\n",
      "Epoch 189, loss: 2.296039\n",
      "Epoch 190, loss: 2.297924\n",
      "Epoch 191, loss: 2.297225\n",
      "Epoch 192, loss: 2.297073\n",
      "Epoch 193, loss: 2.299357\n",
      "Epoch 194, loss: 2.297310\n",
      "Epoch 195, loss: 2.298088\n",
      "Epoch 196, loss: 2.296389\n",
      "Epoch 197, loss: 2.294091\n",
      "Epoch 198, loss: 2.293515\n",
      "Epoch 199, loss: 2.296543\n",
      "Epoch 0, loss: 2.303707\n",
      "Epoch 1, loss: 2.302698\n",
      "Epoch 2, loss: 2.302278\n",
      "Epoch 3, loss: 2.304047\n",
      "Epoch 4, loss: 2.303336\n",
      "Epoch 5, loss: 2.301823\n",
      "Epoch 6, loss: 2.302762\n",
      "Epoch 7, loss: 2.302055\n",
      "Epoch 8, loss: 2.302756\n",
      "Epoch 9, loss: 2.301887\n",
      "Epoch 10, loss: 2.301969\n",
      "Epoch 11, loss: 2.303167\n",
      "Epoch 12, loss: 2.302484\n",
      "Epoch 13, loss: 2.302362\n",
      "Epoch 14, loss: 2.301753\n",
      "Epoch 15, loss: 2.302228\n",
      "Epoch 16, loss: 2.302447\n",
      "Epoch 17, loss: 2.302947\n",
      "Epoch 18, loss: 2.302197\n",
      "Epoch 19, loss: 2.302459\n",
      "Epoch 20, loss: 2.302141\n",
      "Epoch 21, loss: 2.302845\n",
      "Epoch 22, loss: 2.302326\n",
      "Epoch 23, loss: 2.301642\n",
      "Epoch 24, loss: 2.301731\n",
      "Epoch 25, loss: 2.301484\n",
      "Epoch 26, loss: 2.300763\n",
      "Epoch 27, loss: 2.302327\n",
      "Epoch 28, loss: 2.300612\n",
      "Epoch 29, loss: 2.302479\n",
      "Epoch 30, loss: 2.301446\n",
      "Epoch 31, loss: 2.301971\n",
      "Epoch 32, loss: 2.302441\n",
      "Epoch 33, loss: 2.301129\n",
      "Epoch 34, loss: 2.302507\n",
      "Epoch 35, loss: 2.301584\n",
      "Epoch 36, loss: 2.302393\n",
      "Epoch 37, loss: 2.301622\n",
      "Epoch 38, loss: 2.301347\n",
      "Epoch 39, loss: 2.300706\n",
      "Epoch 40, loss: 2.301494\n",
      "Epoch 41, loss: 2.302599\n",
      "Epoch 42, loss: 2.301048\n",
      "Epoch 43, loss: 2.301824\n",
      "Epoch 44, loss: 2.302133\n",
      "Epoch 45, loss: 2.300244\n",
      "Epoch 46, loss: 2.300506\n",
      "Epoch 47, loss: 2.300469\n",
      "Epoch 48, loss: 2.300025\n",
      "Epoch 49, loss: 2.302163\n",
      "Epoch 50, loss: 2.302378\n",
      "Epoch 51, loss: 2.299564\n",
      "Epoch 52, loss: 2.301664\n",
      "Epoch 53, loss: 2.300961\n",
      "Epoch 54, loss: 2.300345\n",
      "Epoch 55, loss: 2.300790\n",
      "Epoch 56, loss: 2.300902\n",
      "Epoch 57, loss: 2.301354\n",
      "Epoch 58, loss: 2.300420\n",
      "Epoch 59, loss: 2.301344\n",
      "Epoch 60, loss: 2.299520\n",
      "Epoch 61, loss: 2.299550\n",
      "Epoch 62, loss: 2.300257\n",
      "Epoch 63, loss: 2.300033\n",
      "Epoch 64, loss: 2.301164\n",
      "Epoch 65, loss: 2.301451\n",
      "Epoch 66, loss: 2.299861\n",
      "Epoch 67, loss: 2.299881\n",
      "Epoch 68, loss: 2.301708\n",
      "Epoch 69, loss: 2.298821\n",
      "Epoch 70, loss: 2.300031\n",
      "Epoch 71, loss: 2.301351\n",
      "Epoch 72, loss: 2.298564\n",
      "Epoch 73, loss: 2.300639\n",
      "Epoch 74, loss: 2.299092\n",
      "Epoch 75, loss: 2.300168\n",
      "Epoch 76, loss: 2.300693\n",
      "Epoch 77, loss: 2.302586\n",
      "Epoch 78, loss: 2.300740\n",
      "Epoch 79, loss: 2.299525\n",
      "Epoch 80, loss: 2.301049\n",
      "Epoch 81, loss: 2.300756\n",
      "Epoch 82, loss: 2.301258\n",
      "Epoch 83, loss: 2.300726\n",
      "Epoch 84, loss: 2.299127\n",
      "Epoch 85, loss: 2.299985\n",
      "Epoch 86, loss: 2.300039\n",
      "Epoch 87, loss: 2.297781\n",
      "Epoch 88, loss: 2.300377\n",
      "Epoch 89, loss: 2.300741\n",
      "Epoch 90, loss: 2.298505\n",
      "Epoch 91, loss: 2.301670\n",
      "Epoch 92, loss: 2.299629\n",
      "Epoch 93, loss: 2.299059\n",
      "Epoch 94, loss: 2.299373\n",
      "Epoch 95, loss: 2.299365\n",
      "Epoch 96, loss: 2.299173\n",
      "Epoch 97, loss: 2.297981\n",
      "Epoch 98, loss: 2.299520\n",
      "Epoch 99, loss: 2.301011\n",
      "Epoch 100, loss: 2.298491\n",
      "Epoch 101, loss: 2.298142\n",
      "Epoch 102, loss: 2.301010\n",
      "Epoch 103, loss: 2.297553\n",
      "Epoch 104, loss: 2.300011\n",
      "Epoch 105, loss: 2.297372\n",
      "Epoch 106, loss: 2.302061\n",
      "Epoch 107, loss: 2.301115\n",
      "Epoch 108, loss: 2.300429\n",
      "Epoch 109, loss: 2.296162\n",
      "Epoch 110, loss: 2.298835\n",
      "Epoch 111, loss: 2.299375\n",
      "Epoch 112, loss: 2.300242\n",
      "Epoch 113, loss: 2.296398\n",
      "Epoch 114, loss: 2.299524\n",
      "Epoch 115, loss: 2.298016\n",
      "Epoch 116, loss: 2.298677\n",
      "Epoch 117, loss: 2.297610\n",
      "Epoch 118, loss: 2.300640\n",
      "Epoch 119, loss: 2.298844\n",
      "Epoch 120, loss: 2.297369\n",
      "Epoch 121, loss: 2.295883\n",
      "Epoch 122, loss: 2.296826\n",
      "Epoch 123, loss: 2.300916\n",
      "Epoch 124, loss: 2.297544\n",
      "Epoch 125, loss: 2.297836\n",
      "Epoch 126, loss: 2.296959\n",
      "Epoch 127, loss: 2.296309\n",
      "Epoch 128, loss: 2.299549\n",
      "Epoch 129, loss: 2.298811\n",
      "Epoch 130, loss: 2.296207\n",
      "Epoch 131, loss: 2.299516\n",
      "Epoch 132, loss: 2.299869\n",
      "Epoch 133, loss: 2.297687\n",
      "Epoch 134, loss: 2.296129\n",
      "Epoch 135, loss: 2.298962\n",
      "Epoch 136, loss: 2.296743\n",
      "Epoch 137, loss: 2.297999\n",
      "Epoch 138, loss: 2.295921\n",
      "Epoch 139, loss: 2.299267\n",
      "Epoch 140, loss: 2.298302\n",
      "Epoch 141, loss: 2.300540\n",
      "Epoch 142, loss: 2.295139\n",
      "Epoch 143, loss: 2.299054\n",
      "Epoch 144, loss: 2.298888\n",
      "Epoch 145, loss: 2.297260\n",
      "Epoch 146, loss: 2.297763\n",
      "Epoch 147, loss: 2.297059\n",
      "Epoch 148, loss: 2.294860\n",
      "Epoch 149, loss: 2.300048\n",
      "Epoch 150, loss: 2.295089\n",
      "Epoch 151, loss: 2.297879\n",
      "Epoch 152, loss: 2.296576\n",
      "Epoch 153, loss: 2.296633\n",
      "Epoch 154, loss: 2.292644\n",
      "Epoch 155, loss: 2.296514\n",
      "Epoch 156, loss: 2.295884\n",
      "Epoch 157, loss: 2.296625\n",
      "Epoch 158, loss: 2.293724\n",
      "Epoch 159, loss: 2.297386\n",
      "Epoch 160, loss: 2.297155\n",
      "Epoch 161, loss: 2.295876\n",
      "Epoch 162, loss: 2.297496\n",
      "Epoch 163, loss: 2.295836\n",
      "Epoch 164, loss: 2.293569\n",
      "Epoch 165, loss: 2.300308\n",
      "Epoch 166, loss: 2.299438\n",
      "Epoch 167, loss: 2.299863\n",
      "Epoch 168, loss: 2.294436\n",
      "Epoch 169, loss: 2.294699\n",
      "Epoch 170, loss: 2.298630\n",
      "Epoch 171, loss: 2.298910\n",
      "Epoch 172, loss: 2.294867\n",
      "Epoch 173, loss: 2.302627\n",
      "Epoch 174, loss: 2.296076\n",
      "Epoch 175, loss: 2.301491\n",
      "Epoch 176, loss: 2.295386\n",
      "Epoch 177, loss: 2.296502\n",
      "Epoch 178, loss: 2.298625\n",
      "Epoch 179, loss: 2.296351\n",
      "Epoch 180, loss: 2.298151\n",
      "Epoch 181, loss: 2.294449\n",
      "Epoch 182, loss: 2.297773\n",
      "Epoch 183, loss: 2.297904\n",
      "Epoch 184, loss: 2.298238\n",
      "Epoch 185, loss: 2.298468\n",
      "Epoch 186, loss: 2.298699\n",
      "Epoch 187, loss: 2.297847\n",
      "Epoch 188, loss: 2.295557\n",
      "Epoch 189, loss: 2.299186\n",
      "Epoch 190, loss: 2.301453\n",
      "Epoch 191, loss: 2.299204\n",
      "Epoch 192, loss: 2.292156\n",
      "Epoch 193, loss: 2.295792\n",
      "Epoch 194, loss: 2.300167\n",
      "Epoch 195, loss: 2.297593\n",
      "Epoch 196, loss: 2.296424\n",
      "Epoch 197, loss: 2.296118\n",
      "Epoch 198, loss: 2.296418\n",
      "Epoch 199, loss: 2.296678\n",
      "Epoch 0, loss: 2.301261\n",
      "Epoch 1, loss: 2.303020\n",
      "Epoch 2, loss: 2.301336\n",
      "Epoch 3, loss: 2.302608\n",
      "Epoch 4, loss: 2.301622\n",
      "Epoch 5, loss: 2.302125\n",
      "Epoch 6, loss: 2.302033\n",
      "Epoch 7, loss: 2.302528\n",
      "Epoch 8, loss: 2.302468\n",
      "Epoch 9, loss: 2.301762\n",
      "Epoch 10, loss: 2.301833\n",
      "Epoch 11, loss: 2.302668\n",
      "Epoch 12, loss: 2.302559\n",
      "Epoch 13, loss: 2.302206\n",
      "Epoch 14, loss: 2.303156\n",
      "Epoch 15, loss: 2.301899\n",
      "Epoch 16, loss: 2.301583\n",
      "Epoch 17, loss: 2.302761\n",
      "Epoch 18, loss: 2.302560\n",
      "Epoch 19, loss: 2.302040\n",
      "Epoch 20, loss: 2.303207\n",
      "Epoch 21, loss: 2.302336\n",
      "Epoch 22, loss: 2.301894\n",
      "Epoch 23, loss: 2.301815\n",
      "Epoch 24, loss: 2.302611\n",
      "Epoch 25, loss: 2.301999\n",
      "Epoch 26, loss: 2.301855\n",
      "Epoch 27, loss: 2.302362\n",
      "Epoch 28, loss: 2.301900\n",
      "Epoch 29, loss: 2.302836\n",
      "Epoch 30, loss: 2.302492\n",
      "Epoch 31, loss: 2.303149\n",
      "Epoch 32, loss: 2.303219\n",
      "Epoch 33, loss: 2.302654\n",
      "Epoch 34, loss: 2.302098\n",
      "Epoch 35, loss: 2.302025\n",
      "Epoch 36, loss: 2.301734\n",
      "Epoch 37, loss: 2.302586\n",
      "Epoch 38, loss: 2.301654\n",
      "Epoch 39, loss: 2.302301\n",
      "Epoch 40, loss: 2.303569\n",
      "Epoch 41, loss: 2.302765\n",
      "Epoch 42, loss: 2.302624\n",
      "Epoch 43, loss: 2.302480\n",
      "Epoch 44, loss: 2.302180\n",
      "Epoch 45, loss: 2.302768\n",
      "Epoch 46, loss: 2.301562\n",
      "Epoch 47, loss: 2.302706\n",
      "Epoch 48, loss: 2.303098\n",
      "Epoch 49, loss: 2.302452\n",
      "Epoch 50, loss: 2.302379\n",
      "Epoch 51, loss: 2.302074\n",
      "Epoch 52, loss: 2.302311\n",
      "Epoch 53, loss: 2.301946\n",
      "Epoch 54, loss: 2.301533\n",
      "Epoch 55, loss: 2.301959\n",
      "Epoch 56, loss: 2.302358\n",
      "Epoch 57, loss: 2.302864\n",
      "Epoch 58, loss: 2.301577\n",
      "Epoch 59, loss: 2.301454\n",
      "Epoch 60, loss: 2.300881\n",
      "Epoch 61, loss: 2.302815\n",
      "Epoch 62, loss: 2.302497\n",
      "Epoch 63, loss: 2.301184\n",
      "Epoch 64, loss: 2.301661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, loss: 2.302423\n",
      "Epoch 66, loss: 2.302085\n",
      "Epoch 67, loss: 2.302803\n",
      "Epoch 68, loss: 2.302787\n",
      "Epoch 69, loss: 2.302288\n",
      "Epoch 70, loss: 2.302800\n",
      "Epoch 71, loss: 2.302908\n",
      "Epoch 72, loss: 2.301976\n",
      "Epoch 73, loss: 2.302064\n",
      "Epoch 74, loss: 2.302415\n",
      "Epoch 75, loss: 2.302766\n",
      "Epoch 76, loss: 2.302109\n",
      "Epoch 77, loss: 2.302201\n",
      "Epoch 78, loss: 2.302845\n",
      "Epoch 79, loss: 2.302262\n",
      "Epoch 80, loss: 2.301830\n",
      "Epoch 81, loss: 2.302268\n",
      "Epoch 82, loss: 2.302302\n",
      "Epoch 83, loss: 2.302187\n",
      "Epoch 84, loss: 2.301557\n",
      "Epoch 85, loss: 2.301854\n",
      "Epoch 86, loss: 2.301722\n",
      "Epoch 87, loss: 2.302010\n",
      "Epoch 88, loss: 2.301745\n",
      "Epoch 89, loss: 2.302077\n",
      "Epoch 90, loss: 2.303090\n",
      "Epoch 91, loss: 2.301870\n",
      "Epoch 92, loss: 2.301735\n",
      "Epoch 93, loss: 2.302428\n",
      "Epoch 94, loss: 2.302341\n",
      "Epoch 95, loss: 2.303045\n",
      "Epoch 96, loss: 2.302455\n",
      "Epoch 97, loss: 2.303327\n",
      "Epoch 98, loss: 2.301866\n",
      "Epoch 99, loss: 2.301442\n",
      "Epoch 100, loss: 2.302014\n",
      "Epoch 101, loss: 2.301505\n",
      "Epoch 102, loss: 2.301667\n",
      "Epoch 103, loss: 2.302200\n",
      "Epoch 104, loss: 2.302598\n",
      "Epoch 105, loss: 2.302046\n",
      "Epoch 106, loss: 2.301569\n",
      "Epoch 107, loss: 2.302197\n",
      "Epoch 108, loss: 2.302066\n",
      "Epoch 109, loss: 2.302842\n",
      "Epoch 110, loss: 2.302039\n",
      "Epoch 111, loss: 2.302165\n",
      "Epoch 112, loss: 2.302439\n",
      "Epoch 113, loss: 2.302562\n",
      "Epoch 114, loss: 2.302379\n",
      "Epoch 115, loss: 2.302238\n",
      "Epoch 116, loss: 2.301070\n",
      "Epoch 117, loss: 2.301976\n",
      "Epoch 118, loss: 2.301688\n",
      "Epoch 119, loss: 2.302345\n",
      "Epoch 120, loss: 2.302213\n",
      "Epoch 121, loss: 2.302811\n",
      "Epoch 122, loss: 2.301488\n",
      "Epoch 123, loss: 2.302259\n",
      "Epoch 124, loss: 2.302357\n",
      "Epoch 125, loss: 2.302662\n",
      "Epoch 126, loss: 2.301220\n",
      "Epoch 127, loss: 2.302515\n",
      "Epoch 128, loss: 2.302431\n",
      "Epoch 129, loss: 2.302640\n",
      "Epoch 130, loss: 2.302184\n",
      "Epoch 131, loss: 2.301608\n",
      "Epoch 132, loss: 2.302115\n",
      "Epoch 133, loss: 2.302307\n",
      "Epoch 134, loss: 2.301984\n",
      "Epoch 135, loss: 2.301911\n",
      "Epoch 136, loss: 2.303378\n",
      "Epoch 137, loss: 2.302277\n",
      "Epoch 138, loss: 2.301631\n",
      "Epoch 139, loss: 2.302261\n",
      "Epoch 140, loss: 2.302311\n",
      "Epoch 141, loss: 2.301567\n",
      "Epoch 142, loss: 2.301997\n",
      "Epoch 143, loss: 2.302132\n",
      "Epoch 144, loss: 2.301504\n",
      "Epoch 145, loss: 2.301193\n",
      "Epoch 146, loss: 2.301809\n",
      "Epoch 147, loss: 2.301122\n",
      "Epoch 148, loss: 2.301945\n",
      "Epoch 149, loss: 2.301694\n",
      "Epoch 150, loss: 2.301981\n",
      "Epoch 151, loss: 2.302674\n",
      "Epoch 152, loss: 2.301806\n",
      "Epoch 153, loss: 2.301308\n",
      "Epoch 154, loss: 2.301115\n",
      "Epoch 155, loss: 2.302028\n",
      "Epoch 156, loss: 2.302612\n",
      "Epoch 157, loss: 2.301675\n",
      "Epoch 158, loss: 2.301509\n",
      "Epoch 159, loss: 2.301175\n",
      "Epoch 160, loss: 2.301968\n",
      "Epoch 161, loss: 2.301946\n",
      "Epoch 162, loss: 2.301827\n",
      "Epoch 163, loss: 2.301159\n",
      "Epoch 164, loss: 2.302106\n",
      "Epoch 165, loss: 2.302609\n",
      "Epoch 166, loss: 2.302051\n",
      "Epoch 167, loss: 2.302371\n",
      "Epoch 168, loss: 2.301635\n",
      "Epoch 169, loss: 2.302885\n",
      "Epoch 170, loss: 2.300865\n",
      "Epoch 171, loss: 2.301819\n",
      "Epoch 172, loss: 2.301766\n",
      "Epoch 173, loss: 2.302442\n",
      "Epoch 174, loss: 2.302647\n",
      "Epoch 175, loss: 2.301399\n",
      "Epoch 176, loss: 2.302248\n",
      "Epoch 177, loss: 2.301684\n",
      "Epoch 178, loss: 2.301822\n",
      "Epoch 179, loss: 2.302432\n",
      "Epoch 180, loss: 2.301732\n",
      "Epoch 181, loss: 2.301889\n",
      "Epoch 182, loss: 2.301297\n",
      "Epoch 183, loss: 2.302226\n",
      "Epoch 184, loss: 2.301096\n",
      "Epoch 185, loss: 2.302218\n",
      "Epoch 186, loss: 2.301296\n",
      "Epoch 187, loss: 2.301688\n",
      "Epoch 188, loss: 2.301440\n",
      "Epoch 189, loss: 2.301887\n",
      "Epoch 190, loss: 2.301478\n",
      "Epoch 191, loss: 2.302310\n",
      "Epoch 192, loss: 2.302318\n",
      "Epoch 193, loss: 2.302775\n",
      "Epoch 194, loss: 2.302026\n",
      "Epoch 195, loss: 2.302192\n",
      "Epoch 196, loss: 2.301034\n",
      "Epoch 197, loss: 2.301798\n",
      "Epoch 198, loss: 2.301840\n",
      "Epoch 199, loss: 2.301161\n",
      "Epoch 0, loss: 2.302426\n",
      "Epoch 1, loss: 2.302836\n",
      "Epoch 2, loss: 2.302387\n",
      "Epoch 3, loss: 2.302228\n",
      "Epoch 4, loss: 2.302120\n",
      "Epoch 5, loss: 2.302088\n",
      "Epoch 6, loss: 2.303027\n",
      "Epoch 7, loss: 2.301651\n",
      "Epoch 8, loss: 2.302188\n",
      "Epoch 9, loss: 2.302750\n",
      "Epoch 10, loss: 2.302534\n",
      "Epoch 11, loss: 2.303649\n",
      "Epoch 12, loss: 2.303238\n",
      "Epoch 13, loss: 2.302425\n",
      "Epoch 14, loss: 2.302978\n",
      "Epoch 15, loss: 2.302657\n",
      "Epoch 16, loss: 2.302394\n",
      "Epoch 17, loss: 2.302113\n",
      "Epoch 18, loss: 2.302922\n",
      "Epoch 19, loss: 2.302069\n",
      "Epoch 20, loss: 2.302732\n",
      "Epoch 21, loss: 2.301888\n",
      "Epoch 22, loss: 2.302666\n",
      "Epoch 23, loss: 2.302719\n",
      "Epoch 24, loss: 2.302706\n",
      "Epoch 25, loss: 2.302619\n",
      "Epoch 26, loss: 2.302701\n",
      "Epoch 27, loss: 2.302906\n",
      "Epoch 28, loss: 2.302967\n",
      "Epoch 29, loss: 2.302993\n",
      "Epoch 30, loss: 2.303274\n",
      "Epoch 31, loss: 2.302383\n",
      "Epoch 32, loss: 2.303171\n",
      "Epoch 33, loss: 2.302664\n",
      "Epoch 34, loss: 2.301887\n",
      "Epoch 35, loss: 2.302394\n",
      "Epoch 36, loss: 2.301708\n",
      "Epoch 37, loss: 2.303156\n",
      "Epoch 38, loss: 2.302976\n",
      "Epoch 39, loss: 2.302622\n",
      "Epoch 40, loss: 2.302419\n",
      "Epoch 41, loss: 2.302326\n",
      "Epoch 42, loss: 2.303173\n",
      "Epoch 43, loss: 2.302012\n",
      "Epoch 44, loss: 2.301507\n",
      "Epoch 45, loss: 2.302642\n",
      "Epoch 46, loss: 2.302184\n",
      "Epoch 47, loss: 2.302863\n",
      "Epoch 48, loss: 2.303465\n",
      "Epoch 49, loss: 2.303051\n",
      "Epoch 50, loss: 2.302638\n",
      "Epoch 51, loss: 2.302696\n",
      "Epoch 52, loss: 2.303036\n",
      "Epoch 53, loss: 2.302021\n",
      "Epoch 54, loss: 2.302445\n",
      "Epoch 55, loss: 2.302403\n",
      "Epoch 56, loss: 2.303290\n",
      "Epoch 57, loss: 2.302983\n",
      "Epoch 58, loss: 2.302620\n",
      "Epoch 59, loss: 2.302735\n",
      "Epoch 60, loss: 2.302077\n",
      "Epoch 61, loss: 2.302183\n",
      "Epoch 62, loss: 2.302981\n",
      "Epoch 63, loss: 2.302255\n",
      "Epoch 64, loss: 2.301012\n",
      "Epoch 65, loss: 2.302582\n",
      "Epoch 66, loss: 2.302798\n",
      "Epoch 67, loss: 2.302175\n",
      "Epoch 68, loss: 2.303056\n",
      "Epoch 69, loss: 2.301241\n",
      "Epoch 70, loss: 2.302427\n",
      "Epoch 71, loss: 2.302905\n",
      "Epoch 72, loss: 2.302287\n",
      "Epoch 73, loss: 2.302656\n",
      "Epoch 74, loss: 2.302763\n",
      "Epoch 75, loss: 2.302252\n",
      "Epoch 76, loss: 2.302464\n",
      "Epoch 77, loss: 2.302432\n",
      "Epoch 78, loss: 2.302408\n",
      "Epoch 79, loss: 2.302373\n",
      "Epoch 80, loss: 2.302501\n",
      "Epoch 81, loss: 2.302447\n",
      "Epoch 82, loss: 2.301287\n",
      "Epoch 83, loss: 2.303099\n",
      "Epoch 84, loss: 2.302732\n",
      "Epoch 85, loss: 2.303159\n",
      "Epoch 86, loss: 2.301891\n",
      "Epoch 87, loss: 2.302471\n",
      "Epoch 88, loss: 2.302066\n",
      "Epoch 89, loss: 2.302962\n",
      "Epoch 90, loss: 2.302195\n",
      "Epoch 91, loss: 2.302736\n",
      "Epoch 92, loss: 2.302196\n",
      "Epoch 93, loss: 2.301245\n",
      "Epoch 94, loss: 2.302404\n",
      "Epoch 95, loss: 2.302274\n",
      "Epoch 96, loss: 2.303056\n",
      "Epoch 97, loss: 2.303275\n",
      "Epoch 98, loss: 2.301711\n",
      "Epoch 99, loss: 2.302607\n",
      "Epoch 100, loss: 2.301511\n",
      "Epoch 101, loss: 2.302226\n",
      "Epoch 102, loss: 2.301780\n",
      "Epoch 103, loss: 2.301918\n",
      "Epoch 104, loss: 2.301920\n",
      "Epoch 105, loss: 2.302790\n",
      "Epoch 106, loss: 2.302662\n",
      "Epoch 107, loss: 2.302143\n",
      "Epoch 108, loss: 2.302205\n",
      "Epoch 109, loss: 2.302980\n",
      "Epoch 110, loss: 2.302250\n",
      "Epoch 111, loss: 2.302551\n",
      "Epoch 112, loss: 2.302450\n",
      "Epoch 113, loss: 2.302589\n",
      "Epoch 114, loss: 2.301857\n",
      "Epoch 115, loss: 2.302541\n",
      "Epoch 116, loss: 2.301263\n",
      "Epoch 117, loss: 2.302703\n",
      "Epoch 118, loss: 2.302095\n",
      "Epoch 119, loss: 2.302053\n",
      "Epoch 120, loss: 2.302543\n",
      "Epoch 121, loss: 2.303160\n",
      "Epoch 122, loss: 2.302061\n",
      "Epoch 123, loss: 2.302774\n",
      "Epoch 124, loss: 2.301831\n",
      "Epoch 125, loss: 2.301629\n",
      "Epoch 126, loss: 2.302491\n",
      "Epoch 127, loss: 2.302361\n",
      "Epoch 128, loss: 2.301499\n",
      "Epoch 129, loss: 2.301845\n",
      "Epoch 130, loss: 2.302517\n",
      "Epoch 131, loss: 2.302131\n",
      "Epoch 132, loss: 2.301362\n",
      "Epoch 133, loss: 2.303580\n",
      "Epoch 134, loss: 2.302373\n",
      "Epoch 135, loss: 2.301963\n",
      "Epoch 136, loss: 2.302576\n",
      "Epoch 137, loss: 2.302271\n",
      "Epoch 138, loss: 2.301390\n",
      "Epoch 139, loss: 2.302498\n",
      "Epoch 140, loss: 2.301976\n",
      "Epoch 141, loss: 2.301570\n",
      "Epoch 142, loss: 2.301440\n",
      "Epoch 143, loss: 2.302669\n",
      "Epoch 144, loss: 2.302221\n",
      "Epoch 145, loss: 2.302595\n",
      "Epoch 146, loss: 2.302111\n",
      "Epoch 147, loss: 2.302274\n",
      "Epoch 148, loss: 2.300793\n",
      "Epoch 149, loss: 2.302168\n",
      "Epoch 150, loss: 2.303680\n",
      "Epoch 151, loss: 2.301499\n",
      "Epoch 152, loss: 2.302336\n",
      "Epoch 153, loss: 2.302088\n",
      "Epoch 154, loss: 2.303050\n",
      "Epoch 155, loss: 2.302006\n",
      "Epoch 156, loss: 2.301698\n",
      "Epoch 157, loss: 2.301667\n",
      "Epoch 158, loss: 2.301585\n",
      "Epoch 159, loss: 2.302277\n",
      "Epoch 160, loss: 2.303332\n",
      "Epoch 161, loss: 2.301971\n",
      "Epoch 162, loss: 2.301658\n",
      "Epoch 163, loss: 2.301888\n",
      "Epoch 164, loss: 2.301486\n",
      "Epoch 165, loss: 2.301919\n",
      "Epoch 166, loss: 2.302885\n",
      "Epoch 167, loss: 2.301513\n",
      "Epoch 168, loss: 2.301727\n",
      "Epoch 169, loss: 2.302173\n",
      "Epoch 170, loss: 2.301251\n",
      "Epoch 171, loss: 2.302722\n",
      "Epoch 172, loss: 2.301209\n",
      "Epoch 173, loss: 2.300510\n",
      "Epoch 174, loss: 2.302603\n",
      "Epoch 175, loss: 2.301515\n",
      "Epoch 176, loss: 2.300914\n",
      "Epoch 177, loss: 2.302597\n",
      "Epoch 178, loss: 2.301778\n",
      "Epoch 179, loss: 2.302060\n",
      "Epoch 180, loss: 2.302069\n",
      "Epoch 181, loss: 2.301474\n",
      "Epoch 182, loss: 2.301968\n",
      "Epoch 183, loss: 2.301897\n",
      "Epoch 184, loss: 2.301651\n",
      "Epoch 185, loss: 2.301248\n",
      "Epoch 186, loss: 2.302794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187, loss: 2.301346\n",
      "Epoch 188, loss: 2.301568\n",
      "Epoch 189, loss: 2.301618\n",
      "Epoch 190, loss: 2.302786\n",
      "Epoch 191, loss: 2.301840\n",
      "Epoch 192, loss: 2.301275\n",
      "Epoch 193, loss: 2.302528\n",
      "Epoch 194, loss: 2.302303\n",
      "Epoch 195, loss: 2.302008\n",
      "Epoch 196, loss: 2.302221\n",
      "Epoch 197, loss: 2.302671\n",
      "Epoch 198, loss: 2.302125\n",
      "Epoch 199, loss: 2.302670\n",
      "Epoch 0, loss: 2.302274\n",
      "Epoch 1, loss: 2.302457\n",
      "Epoch 2, loss: 2.302246\n",
      "Epoch 3, loss: 2.302089\n",
      "Epoch 4, loss: 2.301942\n",
      "Epoch 5, loss: 2.302305\n",
      "Epoch 6, loss: 2.302315\n",
      "Epoch 7, loss: 2.302252\n",
      "Epoch 8, loss: 2.302130\n",
      "Epoch 9, loss: 2.302826\n",
      "Epoch 10, loss: 2.302583\n",
      "Epoch 11, loss: 2.301724\n",
      "Epoch 12, loss: 2.301846\n",
      "Epoch 13, loss: 2.302776\n",
      "Epoch 14, loss: 2.302367\n",
      "Epoch 15, loss: 2.301747\n",
      "Epoch 16, loss: 2.302182\n",
      "Epoch 17, loss: 2.303160\n",
      "Epoch 18, loss: 2.302410\n",
      "Epoch 19, loss: 2.303639\n",
      "Epoch 20, loss: 2.302148\n",
      "Epoch 21, loss: 2.302510\n",
      "Epoch 22, loss: 2.302408\n",
      "Epoch 23, loss: 2.302636\n",
      "Epoch 24, loss: 2.301091\n",
      "Epoch 25, loss: 2.302998\n",
      "Epoch 26, loss: 2.301949\n",
      "Epoch 27, loss: 2.302272\n",
      "Epoch 28, loss: 2.303144\n",
      "Epoch 29, loss: 2.301624\n",
      "Epoch 30, loss: 2.302672\n",
      "Epoch 31, loss: 2.302279\n",
      "Epoch 32, loss: 2.302014\n",
      "Epoch 33, loss: 2.302008\n",
      "Epoch 34, loss: 2.302352\n",
      "Epoch 35, loss: 2.301385\n",
      "Epoch 36, loss: 2.302063\n",
      "Epoch 37, loss: 2.301712\n",
      "Epoch 38, loss: 2.301093\n",
      "Epoch 39, loss: 2.301887\n",
      "Epoch 40, loss: 2.302212\n",
      "Epoch 41, loss: 2.301907\n",
      "Epoch 42, loss: 2.301850\n",
      "Epoch 43, loss: 2.302063\n",
      "Epoch 44, loss: 2.301924\n",
      "Epoch 45, loss: 2.302626\n",
      "Epoch 46, loss: 2.302545\n",
      "Epoch 47, loss: 2.302300\n",
      "Epoch 48, loss: 2.302299\n",
      "Epoch 49, loss: 2.302277\n",
      "Epoch 50, loss: 2.301311\n",
      "Epoch 51, loss: 2.302637\n",
      "Epoch 52, loss: 2.303104\n",
      "Epoch 53, loss: 2.301374\n",
      "Epoch 54, loss: 2.301559\n",
      "Epoch 55, loss: 2.302492\n",
      "Epoch 56, loss: 2.301544\n",
      "Epoch 57, loss: 2.301346\n",
      "Epoch 58, loss: 2.302168\n",
      "Epoch 59, loss: 2.302232\n",
      "Epoch 60, loss: 2.301256\n",
      "Epoch 61, loss: 2.303275\n",
      "Epoch 62, loss: 2.301977\n",
      "Epoch 63, loss: 2.302150\n",
      "Epoch 64, loss: 2.301864\n",
      "Epoch 65, loss: 2.301571\n",
      "Epoch 66, loss: 2.302475\n",
      "Epoch 67, loss: 2.301160\n",
      "Epoch 68, loss: 2.302483\n",
      "Epoch 69, loss: 2.301489\n",
      "Epoch 70, loss: 2.303521\n",
      "Epoch 71, loss: 2.301809\n",
      "Epoch 72, loss: 2.302987\n",
      "Epoch 73, loss: 2.301925\n",
      "Epoch 74, loss: 2.301984\n",
      "Epoch 75, loss: 2.301822\n",
      "Epoch 76, loss: 2.302907\n",
      "Epoch 77, loss: 2.303069\n",
      "Epoch 78, loss: 2.302539\n",
      "Epoch 79, loss: 2.302158\n",
      "Epoch 80, loss: 2.302201\n",
      "Epoch 81, loss: 2.302457\n",
      "Epoch 82, loss: 2.301965\n",
      "Epoch 83, loss: 2.300980\n",
      "Epoch 84, loss: 2.302562\n",
      "Epoch 85, loss: 2.302368\n",
      "Epoch 86, loss: 2.302330\n",
      "Epoch 87, loss: 2.301904\n",
      "Epoch 88, loss: 2.301998\n",
      "Epoch 89, loss: 2.302635\n",
      "Epoch 90, loss: 2.301366\n",
      "Epoch 91, loss: 2.302759\n",
      "Epoch 92, loss: 2.300800\n",
      "Epoch 93, loss: 2.301358\n",
      "Epoch 94, loss: 2.302248\n",
      "Epoch 95, loss: 2.301444\n",
      "Epoch 96, loss: 2.302568\n",
      "Epoch 97, loss: 2.302365\n",
      "Epoch 98, loss: 2.301916\n",
      "Epoch 99, loss: 2.302846\n",
      "Epoch 100, loss: 2.302908\n",
      "Epoch 101, loss: 2.302083\n",
      "Epoch 102, loss: 2.302187\n",
      "Epoch 103, loss: 2.302658\n",
      "Epoch 104, loss: 2.302535\n",
      "Epoch 105, loss: 2.302247\n",
      "Epoch 106, loss: 2.302164\n",
      "Epoch 107, loss: 2.302096\n",
      "Epoch 108, loss: 2.302317\n",
      "Epoch 109, loss: 2.302861\n",
      "Epoch 110, loss: 2.301808\n",
      "Epoch 111, loss: 2.302063\n",
      "Epoch 112, loss: 2.302301\n",
      "Epoch 113, loss: 2.302150\n",
      "Epoch 114, loss: 2.301758\n",
      "Epoch 115, loss: 2.302288\n",
      "Epoch 116, loss: 2.302075\n",
      "Epoch 117, loss: 2.301352\n",
      "Epoch 118, loss: 2.302062\n",
      "Epoch 119, loss: 2.302013\n",
      "Epoch 120, loss: 2.302183\n",
      "Epoch 121, loss: 2.301884\n",
      "Epoch 122, loss: 2.300621\n",
      "Epoch 123, loss: 2.302219\n",
      "Epoch 124, loss: 2.301559\n",
      "Epoch 125, loss: 2.302179\n",
      "Epoch 126, loss: 2.302532\n",
      "Epoch 127, loss: 2.302064\n",
      "Epoch 128, loss: 2.302766\n",
      "Epoch 129, loss: 2.302192\n",
      "Epoch 130, loss: 2.301660\n",
      "Epoch 131, loss: 2.301977\n",
      "Epoch 132, loss: 2.301830\n",
      "Epoch 133, loss: 2.302246\n",
      "Epoch 134, loss: 2.301641\n",
      "Epoch 135, loss: 2.301256\n",
      "Epoch 136, loss: 2.301591\n",
      "Epoch 137, loss: 2.302319\n",
      "Epoch 138, loss: 2.302164\n",
      "Epoch 139, loss: 2.302620\n",
      "Epoch 140, loss: 2.302553\n",
      "Epoch 141, loss: 2.301173\n",
      "Epoch 142, loss: 2.301497\n",
      "Epoch 143, loss: 2.301852\n",
      "Epoch 144, loss: 2.302585\n",
      "Epoch 145, loss: 2.302008\n",
      "Epoch 146, loss: 2.302089\n",
      "Epoch 147, loss: 2.301288\n",
      "Epoch 148, loss: 2.301791\n",
      "Epoch 149, loss: 2.301481\n",
      "Epoch 150, loss: 2.303430\n",
      "Epoch 151, loss: 2.301538\n",
      "Epoch 152, loss: 2.302322\n",
      "Epoch 153, loss: 2.301287\n",
      "Epoch 154, loss: 2.301112\n",
      "Epoch 155, loss: 2.301319\n",
      "Epoch 156, loss: 2.302592\n",
      "Epoch 157, loss: 2.301241\n",
      "Epoch 158, loss: 2.301220\n",
      "Epoch 159, loss: 2.302815\n",
      "Epoch 160, loss: 2.301956\n",
      "Epoch 161, loss: 2.302096\n",
      "Epoch 162, loss: 2.301996\n",
      "Epoch 163, loss: 2.301977\n",
      "Epoch 164, loss: 2.301113\n",
      "Epoch 165, loss: 2.301334\n",
      "Epoch 166, loss: 2.300794\n",
      "Epoch 167, loss: 2.301378\n",
      "Epoch 168, loss: 2.302306\n",
      "Epoch 169, loss: 2.301759\n",
      "Epoch 170, loss: 2.302565\n",
      "Epoch 171, loss: 2.300650\n",
      "Epoch 172, loss: 2.302017\n",
      "Epoch 173, loss: 2.301898\n",
      "Epoch 174, loss: 2.301569\n",
      "Epoch 175, loss: 2.301908\n",
      "Epoch 176, loss: 2.302006\n",
      "Epoch 177, loss: 2.301464\n",
      "Epoch 178, loss: 2.301702\n",
      "Epoch 179, loss: 2.302063\n",
      "Epoch 180, loss: 2.300940\n",
      "Epoch 181, loss: 2.301647\n",
      "Epoch 182, loss: 2.301714\n",
      "Epoch 183, loss: 2.301160\n",
      "Epoch 184, loss: 2.300610\n",
      "Epoch 185, loss: 2.301073\n",
      "Epoch 186, loss: 2.301698\n",
      "Epoch 187, loss: 2.302126\n",
      "Epoch 188, loss: 2.301968\n",
      "Epoch 189, loss: 2.301725\n",
      "Epoch 190, loss: 2.303500\n",
      "Epoch 191, loss: 2.301821\n",
      "Epoch 192, loss: 2.301287\n",
      "Epoch 193, loss: 2.300809\n",
      "Epoch 194, loss: 2.301452\n",
      "Epoch 195, loss: 2.301470\n",
      "Epoch 196, loss: 2.301970\n",
      "Epoch 197, loss: 2.302544\n",
      "Epoch 198, loss: 2.301472\n",
      "Epoch 199, loss: 2.301881\n",
      "Epoch 0, loss: 2.302836\n",
      "Epoch 1, loss: 2.302541\n",
      "Epoch 2, loss: 2.301854\n",
      "Epoch 3, loss: 2.302718\n",
      "Epoch 4, loss: 2.302730\n",
      "Epoch 5, loss: 2.301869\n",
      "Epoch 6, loss: 2.302644\n",
      "Epoch 7, loss: 2.303011\n",
      "Epoch 8, loss: 2.302654\n",
      "Epoch 9, loss: 2.302784\n",
      "Epoch 10, loss: 2.303281\n",
      "Epoch 11, loss: 2.302975\n",
      "Epoch 12, loss: 2.302445\n",
      "Epoch 13, loss: 2.302599\n",
      "Epoch 14, loss: 2.301620\n",
      "Epoch 15, loss: 2.301758\n",
      "Epoch 16, loss: 2.303564\n",
      "Epoch 17, loss: 2.302719\n",
      "Epoch 18, loss: 2.301910\n",
      "Epoch 19, loss: 2.303623\n",
      "Epoch 20, loss: 2.302285\n",
      "Epoch 21, loss: 2.302788\n",
      "Epoch 22, loss: 2.302259\n",
      "Epoch 23, loss: 2.302805\n",
      "Epoch 24, loss: 2.302578\n",
      "Epoch 25, loss: 2.301427\n",
      "Epoch 26, loss: 2.302862\n",
      "Epoch 27, loss: 2.302034\n",
      "Epoch 28, loss: 2.301761\n",
      "Epoch 29, loss: 2.302800\n",
      "Epoch 30, loss: 2.303184\n",
      "Epoch 31, loss: 2.302060\n",
      "Epoch 32, loss: 2.302801\n",
      "Epoch 33, loss: 2.303054\n",
      "Epoch 34, loss: 2.304024\n",
      "Epoch 35, loss: 2.302788\n",
      "Epoch 36, loss: 2.302240\n",
      "Epoch 37, loss: 2.302812\n",
      "Epoch 38, loss: 2.302831\n",
      "Epoch 39, loss: 2.301906\n",
      "Epoch 40, loss: 2.303288\n",
      "Epoch 41, loss: 2.302312\n",
      "Epoch 42, loss: 2.302562\n",
      "Epoch 43, loss: 2.302523\n",
      "Epoch 44, loss: 2.303883\n",
      "Epoch 45, loss: 2.303184\n",
      "Epoch 46, loss: 2.302887\n",
      "Epoch 47, loss: 2.301899\n",
      "Epoch 48, loss: 2.302070\n",
      "Epoch 49, loss: 2.301694\n",
      "Epoch 50, loss: 2.301962\n",
      "Epoch 51, loss: 2.301208\n",
      "Epoch 52, loss: 2.302700\n",
      "Epoch 53, loss: 2.303446\n",
      "Epoch 54, loss: 2.301923\n",
      "Epoch 55, loss: 2.302834\n",
      "Epoch 56, loss: 2.301155\n",
      "Epoch 57, loss: 2.301871\n",
      "Epoch 58, loss: 2.303106\n",
      "Epoch 59, loss: 2.302252\n",
      "Epoch 60, loss: 2.302455\n",
      "Epoch 61, loss: 2.302966\n",
      "Epoch 62, loss: 2.302735\n",
      "Epoch 63, loss: 2.301779\n",
      "Epoch 64, loss: 2.302932\n",
      "Epoch 65, loss: 2.302072\n",
      "Epoch 66, loss: 2.303074\n",
      "Epoch 67, loss: 2.301958\n",
      "Epoch 68, loss: 2.302415\n",
      "Epoch 69, loss: 2.302056\n",
      "Epoch 70, loss: 2.303400\n",
      "Epoch 71, loss: 2.301701\n",
      "Epoch 72, loss: 2.302924\n",
      "Epoch 73, loss: 2.303689\n",
      "Epoch 74, loss: 2.303247\n",
      "Epoch 75, loss: 2.302264\n",
      "Epoch 76, loss: 2.302391\n",
      "Epoch 77, loss: 2.302411\n",
      "Epoch 78, loss: 2.303128\n",
      "Epoch 79, loss: 2.301240\n",
      "Epoch 80, loss: 2.302656\n",
      "Epoch 81, loss: 2.301572\n",
      "Epoch 82, loss: 2.302953\n",
      "Epoch 83, loss: 2.303002\n",
      "Epoch 84, loss: 2.303440\n",
      "Epoch 85, loss: 2.303525\n",
      "Epoch 86, loss: 2.302282\n",
      "Epoch 87, loss: 2.303173\n",
      "Epoch 88, loss: 2.303376\n",
      "Epoch 89, loss: 2.301614\n",
      "Epoch 90, loss: 2.302385\n",
      "Epoch 91, loss: 2.301775\n",
      "Epoch 92, loss: 2.303639\n",
      "Epoch 93, loss: 2.302480\n",
      "Epoch 94, loss: 2.303484\n",
      "Epoch 95, loss: 2.301795\n",
      "Epoch 96, loss: 2.304050\n",
      "Epoch 97, loss: 2.301822\n",
      "Epoch 98, loss: 2.302766\n",
      "Epoch 99, loss: 2.303006\n",
      "Epoch 100, loss: 2.302751\n",
      "Epoch 101, loss: 2.302033\n",
      "Epoch 102, loss: 2.303222\n",
      "Epoch 103, loss: 2.301338\n",
      "Epoch 104, loss: 2.302043\n",
      "Epoch 105, loss: 2.302430\n",
      "Epoch 106, loss: 2.303415\n",
      "Epoch 107, loss: 2.301938\n",
      "Epoch 108, loss: 2.303415\n",
      "Epoch 109, loss: 2.302363\n",
      "Epoch 110, loss: 2.303017\n",
      "Epoch 111, loss: 2.301654\n",
      "Epoch 112, loss: 2.303245\n",
      "Epoch 113, loss: 2.301505\n",
      "Epoch 114, loss: 2.303708\n",
      "Epoch 115, loss: 2.301263\n",
      "Epoch 116, loss: 2.302390\n",
      "Epoch 117, loss: 2.301568\n",
      "Epoch 118, loss: 2.302741\n",
      "Epoch 119, loss: 2.302088\n",
      "Epoch 120, loss: 2.302732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, loss: 2.301938\n",
      "Epoch 122, loss: 2.302720\n",
      "Epoch 123, loss: 2.302904\n",
      "Epoch 124, loss: 2.303232\n",
      "Epoch 125, loss: 2.303229\n",
      "Epoch 126, loss: 2.301853\n",
      "Epoch 127, loss: 2.302252\n",
      "Epoch 128, loss: 2.302009\n",
      "Epoch 129, loss: 2.302979\n",
      "Epoch 130, loss: 2.302570\n",
      "Epoch 131, loss: 2.302309\n",
      "Epoch 132, loss: 2.303468\n",
      "Epoch 133, loss: 2.302165\n",
      "Epoch 134, loss: 2.302888\n",
      "Epoch 135, loss: 2.303577\n",
      "Epoch 136, loss: 2.302612\n",
      "Epoch 137, loss: 2.302349\n",
      "Epoch 138, loss: 2.302683\n",
      "Epoch 139, loss: 2.301655\n",
      "Epoch 140, loss: 2.302692\n",
      "Epoch 141, loss: 2.303351\n",
      "Epoch 142, loss: 2.302675\n",
      "Epoch 143, loss: 2.302933\n",
      "Epoch 144, loss: 2.302223\n",
      "Epoch 145, loss: 2.302559\n",
      "Epoch 146, loss: 2.302394\n",
      "Epoch 147, loss: 2.302297\n",
      "Epoch 148, loss: 2.302362\n",
      "Epoch 149, loss: 2.302283\n",
      "Epoch 150, loss: 2.303053\n",
      "Epoch 151, loss: 2.303514\n",
      "Epoch 152, loss: 2.302235\n",
      "Epoch 153, loss: 2.302753\n",
      "Epoch 154, loss: 2.303692\n",
      "Epoch 155, loss: 2.302688\n",
      "Epoch 156, loss: 2.304082\n",
      "Epoch 157, loss: 2.303125\n",
      "Epoch 158, loss: 2.302525\n",
      "Epoch 159, loss: 2.302844\n",
      "Epoch 160, loss: 2.302375\n",
      "Epoch 161, loss: 2.302723\n",
      "Epoch 162, loss: 2.302160\n",
      "Epoch 163, loss: 2.304126\n",
      "Epoch 164, loss: 2.303464\n",
      "Epoch 165, loss: 2.303564\n",
      "Epoch 166, loss: 2.301929\n",
      "Epoch 167, loss: 2.301609\n",
      "Epoch 168, loss: 2.303598\n",
      "Epoch 169, loss: 2.302700\n",
      "Epoch 170, loss: 2.303038\n",
      "Epoch 171, loss: 2.302317\n",
      "Epoch 172, loss: 2.303084\n",
      "Epoch 173, loss: 2.303341\n",
      "Epoch 174, loss: 2.303503\n",
      "Epoch 175, loss: 2.302680\n",
      "Epoch 176, loss: 2.301425\n",
      "Epoch 177, loss: 2.302131\n",
      "Epoch 178, loss: 2.303176\n",
      "Epoch 179, loss: 2.302261\n",
      "Epoch 180, loss: 2.301983\n",
      "Epoch 181, loss: 2.303071\n",
      "Epoch 182, loss: 2.301941\n",
      "Epoch 183, loss: 2.303297\n",
      "Epoch 184, loss: 2.303242\n",
      "Epoch 185, loss: 2.302145\n",
      "Epoch 186, loss: 2.301243\n",
      "Epoch 187, loss: 2.301544\n",
      "Epoch 188, loss: 2.300999\n",
      "Epoch 189, loss: 2.302481\n",
      "Epoch 190, loss: 2.301971\n",
      "Epoch 191, loss: 2.302645\n",
      "Epoch 192, loss: 2.302469\n",
      "Epoch 193, loss: 2.302877\n",
      "Epoch 194, loss: 2.303555\n",
      "Epoch 195, loss: 2.302569\n",
      "Epoch 196, loss: 2.302652\n",
      "Epoch 197, loss: 2.302348\n",
      "Epoch 198, loss: 2.302712\n",
      "Epoch 199, loss: 2.302856\n",
      "Epoch 0, loss: 2.303794\n",
      "Epoch 1, loss: 2.302876\n",
      "Epoch 2, loss: 2.303373\n",
      "Epoch 3, loss: 2.303303\n",
      "Epoch 4, loss: 2.302893\n",
      "Epoch 5, loss: 2.302986\n",
      "Epoch 6, loss: 2.303299\n",
      "Epoch 7, loss: 2.303161\n",
      "Epoch 8, loss: 2.303121\n",
      "Epoch 9, loss: 2.302669\n",
      "Epoch 10, loss: 2.303700\n",
      "Epoch 11, loss: 2.302716\n",
      "Epoch 12, loss: 2.303539\n",
      "Epoch 13, loss: 2.303441\n",
      "Epoch 14, loss: 2.302932\n",
      "Epoch 15, loss: 2.303405\n",
      "Epoch 16, loss: 2.302785\n",
      "Epoch 17, loss: 2.302451\n",
      "Epoch 18, loss: 2.302105\n",
      "Epoch 19, loss: 2.302276\n",
      "Epoch 20, loss: 2.303030\n",
      "Epoch 21, loss: 2.302945\n",
      "Epoch 22, loss: 2.302499\n",
      "Epoch 23, loss: 2.302745\n",
      "Epoch 24, loss: 2.302518\n",
      "Epoch 25, loss: 2.302311\n",
      "Epoch 26, loss: 2.302325\n",
      "Epoch 27, loss: 2.302193\n",
      "Epoch 28, loss: 2.303023\n",
      "Epoch 29, loss: 2.303079\n",
      "Epoch 30, loss: 2.302971\n",
      "Epoch 31, loss: 2.301906\n",
      "Epoch 32, loss: 2.302681\n",
      "Epoch 33, loss: 2.303054\n",
      "Epoch 34, loss: 2.302438\n",
      "Epoch 35, loss: 2.302058\n",
      "Epoch 36, loss: 2.302743\n",
      "Epoch 37, loss: 2.302255\n",
      "Epoch 38, loss: 2.303428\n",
      "Epoch 39, loss: 2.302951\n",
      "Epoch 40, loss: 2.303074\n",
      "Epoch 41, loss: 2.303056\n",
      "Epoch 42, loss: 2.303841\n",
      "Epoch 43, loss: 2.302802\n",
      "Epoch 44, loss: 2.302952\n",
      "Epoch 45, loss: 2.302304\n",
      "Epoch 46, loss: 2.302833\n",
      "Epoch 47, loss: 2.302531\n",
      "Epoch 48, loss: 2.302417\n",
      "Epoch 49, loss: 2.303135\n",
      "Epoch 50, loss: 2.303212\n",
      "Epoch 51, loss: 2.303684\n",
      "Epoch 52, loss: 2.301977\n",
      "Epoch 53, loss: 2.303568\n",
      "Epoch 54, loss: 2.301859\n",
      "Epoch 55, loss: 2.301770\n",
      "Epoch 56, loss: 2.302287\n",
      "Epoch 57, loss: 2.302792\n",
      "Epoch 58, loss: 2.302604\n",
      "Epoch 59, loss: 2.302631\n",
      "Epoch 60, loss: 2.302563\n",
      "Epoch 61, loss: 2.302958\n",
      "Epoch 62, loss: 2.302616\n",
      "Epoch 63, loss: 2.302811\n",
      "Epoch 64, loss: 2.302826\n",
      "Epoch 65, loss: 2.302991\n",
      "Epoch 66, loss: 2.302521\n",
      "Epoch 67, loss: 2.302247\n",
      "Epoch 68, loss: 2.303407\n",
      "Epoch 69, loss: 2.302981\n",
      "Epoch 70, loss: 2.302292\n",
      "Epoch 71, loss: 2.302756\n",
      "Epoch 72, loss: 2.301874\n",
      "Epoch 73, loss: 2.301714\n",
      "Epoch 74, loss: 2.302971\n",
      "Epoch 75, loss: 2.303365\n",
      "Epoch 76, loss: 2.303349\n",
      "Epoch 77, loss: 2.303533\n",
      "Epoch 78, loss: 2.302914\n",
      "Epoch 79, loss: 2.302233\n",
      "Epoch 80, loss: 2.302829\n",
      "Epoch 81, loss: 2.302234\n",
      "Epoch 82, loss: 2.301673\n",
      "Epoch 83, loss: 2.302639\n",
      "Epoch 84, loss: 2.303301\n",
      "Epoch 85, loss: 2.302888\n",
      "Epoch 86, loss: 2.302765\n",
      "Epoch 87, loss: 2.302477\n",
      "Epoch 88, loss: 2.302562\n",
      "Epoch 89, loss: 2.302181\n",
      "Epoch 90, loss: 2.302571\n",
      "Epoch 91, loss: 2.302561\n",
      "Epoch 92, loss: 2.302616\n",
      "Epoch 93, loss: 2.302839\n",
      "Epoch 94, loss: 2.302448\n",
      "Epoch 95, loss: 2.302711\n",
      "Epoch 96, loss: 2.302121\n",
      "Epoch 97, loss: 2.302206\n",
      "Epoch 98, loss: 2.302416\n",
      "Epoch 99, loss: 2.302933\n",
      "Epoch 100, loss: 2.302408\n",
      "Epoch 101, loss: 2.303283\n",
      "Epoch 102, loss: 2.303065\n",
      "Epoch 103, loss: 2.302913\n",
      "Epoch 104, loss: 2.302299\n",
      "Epoch 105, loss: 2.302972\n",
      "Epoch 106, loss: 2.302967\n",
      "Epoch 107, loss: 2.302842\n",
      "Epoch 108, loss: 2.302892\n",
      "Epoch 109, loss: 2.302935\n",
      "Epoch 110, loss: 2.303348\n",
      "Epoch 111, loss: 2.302797\n",
      "Epoch 112, loss: 2.302700\n",
      "Epoch 113, loss: 2.302536\n",
      "Epoch 114, loss: 2.304010\n",
      "Epoch 115, loss: 2.303299\n",
      "Epoch 116, loss: 2.302046\n",
      "Epoch 117, loss: 2.302835\n",
      "Epoch 118, loss: 2.302249\n",
      "Epoch 119, loss: 2.303289\n",
      "Epoch 120, loss: 2.303753\n",
      "Epoch 121, loss: 2.303823\n",
      "Epoch 122, loss: 2.303395\n",
      "Epoch 123, loss: 2.302476\n",
      "Epoch 124, loss: 2.303589\n",
      "Epoch 125, loss: 2.303469\n",
      "Epoch 126, loss: 2.302389\n",
      "Epoch 127, loss: 2.303247\n",
      "Epoch 128, loss: 2.303434\n",
      "Epoch 129, loss: 2.302848\n",
      "Epoch 130, loss: 2.302084\n",
      "Epoch 131, loss: 2.303095\n",
      "Epoch 132, loss: 2.303013\n",
      "Epoch 133, loss: 2.303212\n",
      "Epoch 134, loss: 2.302923\n",
      "Epoch 135, loss: 2.302038\n",
      "Epoch 136, loss: 2.302878\n",
      "Epoch 137, loss: 2.302413\n",
      "Epoch 138, loss: 2.302737\n",
      "Epoch 139, loss: 2.303322\n",
      "Epoch 140, loss: 2.303049\n",
      "Epoch 141, loss: 2.303904\n",
      "Epoch 142, loss: 2.304169\n",
      "Epoch 143, loss: 2.302983\n",
      "Epoch 144, loss: 2.303499\n",
      "Epoch 145, loss: 2.302982\n",
      "Epoch 146, loss: 2.302961\n",
      "Epoch 147, loss: 2.303243\n",
      "Epoch 148, loss: 2.302747\n",
      "Epoch 149, loss: 2.302559\n",
      "Epoch 150, loss: 2.303236\n",
      "Epoch 151, loss: 2.302262\n",
      "Epoch 152, loss: 2.302119\n",
      "Epoch 153, loss: 2.303521\n",
      "Epoch 154, loss: 2.303340\n",
      "Epoch 155, loss: 2.302856\n",
      "Epoch 156, loss: 2.302201\n",
      "Epoch 157, loss: 2.302474\n",
      "Epoch 158, loss: 2.302406\n",
      "Epoch 159, loss: 2.302787\n",
      "Epoch 160, loss: 2.302898\n",
      "Epoch 161, loss: 2.302892\n",
      "Epoch 162, loss: 2.303451\n",
      "Epoch 163, loss: 2.303486\n",
      "Epoch 164, loss: 2.301936\n",
      "Epoch 165, loss: 2.301885\n",
      "Epoch 166, loss: 2.303903\n",
      "Epoch 167, loss: 2.303551\n",
      "Epoch 168, loss: 2.303322\n",
      "Epoch 169, loss: 2.303005\n",
      "Epoch 170, loss: 2.303202\n",
      "Epoch 171, loss: 2.303256\n",
      "Epoch 172, loss: 2.302068\n",
      "Epoch 173, loss: 2.302648\n",
      "Epoch 174, loss: 2.302489\n",
      "Epoch 175, loss: 2.302527\n",
      "Epoch 176, loss: 2.303198\n",
      "Epoch 177, loss: 2.302541\n",
      "Epoch 178, loss: 2.301627\n",
      "Epoch 179, loss: 2.301987\n",
      "Epoch 180, loss: 2.302696\n",
      "Epoch 181, loss: 2.302235\n",
      "Epoch 182, loss: 2.302555\n",
      "Epoch 183, loss: 2.303356\n",
      "Epoch 184, loss: 2.302421\n",
      "Epoch 185, loss: 2.303239\n",
      "Epoch 186, loss: 2.302731\n",
      "Epoch 187, loss: 2.301994\n",
      "Epoch 188, loss: 2.302712\n",
      "Epoch 189, loss: 2.303150\n",
      "Epoch 190, loss: 2.303417\n",
      "Epoch 191, loss: 2.303301\n",
      "Epoch 192, loss: 2.302394\n",
      "Epoch 193, loss: 2.302386\n",
      "Epoch 194, loss: 2.303166\n",
      "Epoch 195, loss: 2.303021\n",
      "Epoch 196, loss: 2.302779\n",
      "Epoch 197, loss: 2.302486\n",
      "Epoch 198, loss: 2.302440\n",
      "Epoch 199, loss: 2.303166\n",
      "Epoch 0, loss: 2.302542\n",
      "Epoch 1, loss: 2.302935\n",
      "Epoch 2, loss: 2.303793\n",
      "Epoch 3, loss: 2.303358\n",
      "Epoch 4, loss: 2.304386\n",
      "Epoch 5, loss: 2.302570\n",
      "Epoch 6, loss: 2.302380\n",
      "Epoch 7, loss: 2.303478\n",
      "Epoch 8, loss: 2.302028\n",
      "Epoch 9, loss: 2.303192\n",
      "Epoch 10, loss: 2.302517\n",
      "Epoch 11, loss: 2.302403\n",
      "Epoch 12, loss: 2.302799\n",
      "Epoch 13, loss: 2.302512\n",
      "Epoch 14, loss: 2.301864\n",
      "Epoch 15, loss: 2.302406\n",
      "Epoch 16, loss: 2.302729\n",
      "Epoch 17, loss: 2.302820\n",
      "Epoch 18, loss: 2.303563\n",
      "Epoch 19, loss: 2.304793\n",
      "Epoch 20, loss: 2.303259\n",
      "Epoch 21, loss: 2.304266\n",
      "Epoch 22, loss: 2.302606\n",
      "Epoch 23, loss: 2.302916\n",
      "Epoch 24, loss: 2.303736\n",
      "Epoch 25, loss: 2.304193\n",
      "Epoch 26, loss: 2.303663\n",
      "Epoch 27, loss: 2.302530\n",
      "Epoch 28, loss: 2.303196\n",
      "Epoch 29, loss: 2.303762\n",
      "Epoch 30, loss: 2.301490\n",
      "Epoch 31, loss: 2.302832\n",
      "Epoch 32, loss: 2.303312\n",
      "Epoch 33, loss: 2.302314\n",
      "Epoch 34, loss: 2.303013\n",
      "Epoch 35, loss: 2.302996\n",
      "Epoch 36, loss: 2.303526\n",
      "Epoch 37, loss: 2.302325\n",
      "Epoch 38, loss: 2.302209\n",
      "Epoch 39, loss: 2.302683\n",
      "Epoch 40, loss: 2.304292\n",
      "Epoch 41, loss: 2.303390\n",
      "Epoch 42, loss: 2.302135\n",
      "Epoch 43, loss: 2.303975\n",
      "Epoch 44, loss: 2.302088\n",
      "Epoch 45, loss: 2.303179\n",
      "Epoch 46, loss: 2.302462\n",
      "Epoch 47, loss: 2.303397\n",
      "Epoch 48, loss: 2.303933\n",
      "Epoch 49, loss: 2.302914\n",
      "Epoch 50, loss: 2.302732\n",
      "Epoch 51, loss: 2.303474\n",
      "Epoch 52, loss: 2.303589\n",
      "Epoch 53, loss: 2.302929\n",
      "Epoch 54, loss: 2.303174\n",
      "Epoch 55, loss: 2.302589\n",
      "Epoch 56, loss: 2.302189\n",
      "Epoch 57, loss: 2.303705\n",
      "Epoch 58, loss: 2.302778\n",
      "Epoch 59, loss: 2.302572\n",
      "Epoch 60, loss: 2.303115\n",
      "Epoch 61, loss: 2.302676\n",
      "Epoch 62, loss: 2.301817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, loss: 2.303377\n",
      "Epoch 64, loss: 2.302216\n",
      "Epoch 65, loss: 2.303709\n",
      "Epoch 66, loss: 2.302404\n",
      "Epoch 67, loss: 2.303248\n",
      "Epoch 68, loss: 2.303911\n",
      "Epoch 69, loss: 2.302725\n",
      "Epoch 70, loss: 2.303569\n",
      "Epoch 71, loss: 2.303526\n",
      "Epoch 72, loss: 2.302528\n",
      "Epoch 73, loss: 2.302966\n",
      "Epoch 74, loss: 2.301914\n",
      "Epoch 75, loss: 2.303364\n",
      "Epoch 76, loss: 2.302070\n",
      "Epoch 77, loss: 2.302771\n",
      "Epoch 78, loss: 2.303049\n",
      "Epoch 79, loss: 2.303083\n",
      "Epoch 80, loss: 2.303256\n",
      "Epoch 81, loss: 2.303390\n",
      "Epoch 82, loss: 2.302981\n",
      "Epoch 83, loss: 2.303302\n",
      "Epoch 84, loss: 2.303411\n",
      "Epoch 85, loss: 2.301705\n",
      "Epoch 86, loss: 2.303069\n",
      "Epoch 87, loss: 2.303759\n",
      "Epoch 88, loss: 2.303347\n",
      "Epoch 89, loss: 2.302698\n",
      "Epoch 90, loss: 2.302785\n",
      "Epoch 91, loss: 2.301969\n",
      "Epoch 92, loss: 2.304149\n",
      "Epoch 93, loss: 2.302699\n",
      "Epoch 94, loss: 2.302422\n",
      "Epoch 95, loss: 2.303526\n",
      "Epoch 96, loss: 2.302992\n",
      "Epoch 97, loss: 2.303514\n",
      "Epoch 98, loss: 2.301987\n",
      "Epoch 99, loss: 2.302922\n",
      "Epoch 100, loss: 2.301646\n",
      "Epoch 101, loss: 2.303333\n",
      "Epoch 102, loss: 2.302049\n",
      "Epoch 103, loss: 2.302295\n",
      "Epoch 104, loss: 2.301964\n",
      "Epoch 105, loss: 2.302783\n",
      "Epoch 106, loss: 2.302194\n",
      "Epoch 107, loss: 2.303245\n",
      "Epoch 108, loss: 2.303110\n",
      "Epoch 109, loss: 2.303052\n",
      "Epoch 110, loss: 2.301933\n",
      "Epoch 111, loss: 2.301841\n",
      "Epoch 112, loss: 2.302723\n",
      "Epoch 113, loss: 2.303127\n",
      "Epoch 114, loss: 2.304063\n",
      "Epoch 115, loss: 2.303660\n",
      "Epoch 116, loss: 2.303365\n",
      "Epoch 117, loss: 2.302156\n",
      "Epoch 118, loss: 2.303077\n",
      "Epoch 119, loss: 2.303379\n",
      "Epoch 120, loss: 2.302456\n",
      "Epoch 121, loss: 2.303445\n",
      "Epoch 122, loss: 2.301970\n",
      "Epoch 123, loss: 2.302595\n",
      "Epoch 124, loss: 2.303235\n",
      "Epoch 125, loss: 2.302873\n",
      "Epoch 126, loss: 2.301330\n",
      "Epoch 127, loss: 2.302995\n",
      "Epoch 128, loss: 2.303039\n",
      "Epoch 129, loss: 2.302775\n",
      "Epoch 130, loss: 2.302885\n",
      "Epoch 131, loss: 2.302685\n",
      "Epoch 132, loss: 2.302208\n",
      "Epoch 133, loss: 2.301883\n",
      "Epoch 134, loss: 2.304107\n",
      "Epoch 135, loss: 2.303199\n",
      "Epoch 136, loss: 2.301972\n",
      "Epoch 137, loss: 2.302231\n",
      "Epoch 138, loss: 2.303544\n",
      "Epoch 139, loss: 2.303342\n",
      "Epoch 140, loss: 2.303170\n",
      "Epoch 141, loss: 2.302441\n",
      "Epoch 142, loss: 2.302778\n",
      "Epoch 143, loss: 2.303342\n",
      "Epoch 144, loss: 2.302430\n",
      "Epoch 145, loss: 2.302406\n",
      "Epoch 146, loss: 2.303546\n",
      "Epoch 147, loss: 2.302654\n",
      "Epoch 148, loss: 2.302960\n",
      "Epoch 149, loss: 2.302903\n",
      "Epoch 150, loss: 2.302952\n",
      "Epoch 151, loss: 2.303221\n",
      "Epoch 152, loss: 2.303202\n",
      "Epoch 153, loss: 2.303097\n",
      "Epoch 154, loss: 2.303044\n",
      "Epoch 155, loss: 2.301600\n",
      "Epoch 156, loss: 2.302685\n",
      "Epoch 157, loss: 2.301811\n",
      "Epoch 158, loss: 2.303934\n",
      "Epoch 159, loss: 2.302989\n",
      "Epoch 160, loss: 2.303765\n",
      "Epoch 161, loss: 2.302438\n",
      "Epoch 162, loss: 2.303551\n",
      "Epoch 163, loss: 2.303172\n",
      "Epoch 164, loss: 2.302485\n",
      "Epoch 165, loss: 2.302888\n",
      "Epoch 166, loss: 2.302513\n",
      "Epoch 167, loss: 2.303345\n",
      "Epoch 168, loss: 2.302911\n",
      "Epoch 169, loss: 2.302065\n",
      "Epoch 170, loss: 2.304155\n",
      "Epoch 171, loss: 2.302404\n",
      "Epoch 172, loss: 2.302489\n",
      "Epoch 173, loss: 2.304072\n",
      "Epoch 174, loss: 2.302252\n",
      "Epoch 175, loss: 2.303706\n",
      "Epoch 176, loss: 2.303133\n",
      "Epoch 177, loss: 2.303406\n",
      "Epoch 178, loss: 2.303159\n",
      "Epoch 179, loss: 2.302295\n",
      "Epoch 180, loss: 2.301853\n",
      "Epoch 181, loss: 2.302632\n",
      "Epoch 182, loss: 2.302795\n",
      "Epoch 183, loss: 2.303869\n",
      "Epoch 184, loss: 2.303311\n",
      "Epoch 185, loss: 2.302047\n",
      "Epoch 186, loss: 2.303354\n",
      "Epoch 187, loss: 2.302356\n",
      "Epoch 188, loss: 2.303205\n",
      "Epoch 189, loss: 2.302586\n",
      "Epoch 190, loss: 2.302586\n",
      "Epoch 191, loss: 2.301889\n",
      "Epoch 192, loss: 2.302718\n",
      "Epoch 193, loss: 2.303245\n",
      "Epoch 194, loss: 2.302923\n",
      "Epoch 195, loss: 2.303761\n",
      "Epoch 196, loss: 2.303054\n",
      "Epoch 197, loss: 2.303107\n",
      "Epoch 198, loss: 2.301778\n",
      "Epoch 199, loss: 2.302391\n",
      "Epoch 0, loss: 2.302277\n",
      "Epoch 1, loss: 2.308734\n",
      "Epoch 2, loss: 2.299614\n",
      "Epoch 3, loss: 2.302560\n",
      "Epoch 4, loss: 2.298338\n",
      "Epoch 5, loss: 2.287167\n",
      "Epoch 6, loss: 2.302290\n",
      "Epoch 7, loss: 2.300669\n",
      "Epoch 8, loss: 2.297864\n",
      "Epoch 9, loss: 2.294192\n",
      "Epoch 10, loss: 2.292616\n",
      "Epoch 11, loss: 2.279340\n",
      "Epoch 12, loss: 2.302209\n",
      "Epoch 13, loss: 2.276167\n",
      "Epoch 14, loss: 2.311855\n",
      "Epoch 15, loss: 2.270305\n",
      "Epoch 16, loss: 2.266621\n",
      "Epoch 17, loss: 2.293125\n",
      "Epoch 18, loss: 2.267404\n",
      "Epoch 19, loss: 2.268338\n",
      "Epoch 20, loss: 2.274469\n",
      "Epoch 21, loss: 2.268768\n",
      "Epoch 22, loss: 2.272355\n",
      "Epoch 23, loss: 2.250586\n",
      "Epoch 24, loss: 2.258025\n",
      "Epoch 25, loss: 2.265952\n",
      "Epoch 26, loss: 2.252310\n",
      "Epoch 27, loss: 2.258922\n",
      "Epoch 28, loss: 2.270635\n",
      "Epoch 29, loss: 2.238876\n",
      "Epoch 30, loss: 2.270454\n",
      "Epoch 31, loss: 2.262326\n",
      "Epoch 32, loss: 2.244674\n",
      "Epoch 33, loss: 2.265797\n",
      "Epoch 34, loss: 2.281513\n",
      "Epoch 35, loss: 2.253191\n",
      "Epoch 36, loss: 2.254725\n",
      "Epoch 37, loss: 2.269651\n",
      "Epoch 38, loss: 2.265266\n",
      "Epoch 39, loss: 2.247168\n",
      "Epoch 40, loss: 2.257889\n",
      "Epoch 41, loss: 2.253012\n",
      "Epoch 42, loss: 2.264490\n",
      "Epoch 43, loss: 2.243133\n",
      "Epoch 44, loss: 2.252502\n",
      "Epoch 45, loss: 2.240509\n",
      "Epoch 46, loss: 2.250594\n",
      "Epoch 47, loss: 2.237495\n",
      "Epoch 48, loss: 2.249170\n",
      "Epoch 49, loss: 2.245123\n",
      "Epoch 50, loss: 2.250096\n",
      "Epoch 51, loss: 2.227087\n",
      "Epoch 52, loss: 2.245226\n",
      "Epoch 53, loss: 2.238276\n",
      "Epoch 54, loss: 2.231370\n",
      "Epoch 55, loss: 2.233608\n",
      "Epoch 56, loss: 2.234758\n",
      "Epoch 57, loss: 2.235383\n",
      "Epoch 58, loss: 2.237911\n",
      "Epoch 59, loss: 2.249152\n",
      "Epoch 60, loss: 2.223932\n",
      "Epoch 61, loss: 2.250091\n",
      "Epoch 62, loss: 2.216970\n",
      "Epoch 63, loss: 2.235961\n",
      "Epoch 64, loss: 2.211949\n",
      "Epoch 65, loss: 2.230793\n",
      "Epoch 66, loss: 2.251059\n",
      "Epoch 67, loss: 2.252608\n",
      "Epoch 68, loss: 2.215892\n",
      "Epoch 69, loss: 2.212611\n",
      "Epoch 70, loss: 2.207017\n",
      "Epoch 71, loss: 2.230589\n",
      "Epoch 72, loss: 2.227798\n",
      "Epoch 73, loss: 2.227962\n",
      "Epoch 74, loss: 2.234668\n",
      "Epoch 75, loss: 2.214631\n",
      "Epoch 76, loss: 2.196239\n",
      "Epoch 77, loss: 2.230472\n",
      "Epoch 78, loss: 2.201037\n",
      "Epoch 79, loss: 2.213511\n",
      "Epoch 80, loss: 2.227000\n",
      "Epoch 81, loss: 2.179881\n",
      "Epoch 82, loss: 2.207629\n",
      "Epoch 83, loss: 2.207334\n",
      "Epoch 84, loss: 2.219856\n",
      "Epoch 85, loss: 2.200641\n",
      "Epoch 86, loss: 2.200145\n",
      "Epoch 87, loss: 2.271643\n",
      "Epoch 88, loss: 2.199829\n",
      "Epoch 89, loss: 2.208833\n",
      "Epoch 90, loss: 2.226978\n",
      "Epoch 91, loss: 2.234750\n",
      "Epoch 92, loss: 2.195242\n",
      "Epoch 93, loss: 2.224347\n",
      "Epoch 94, loss: 2.202089\n",
      "Epoch 95, loss: 2.191448\n",
      "Epoch 96, loss: 2.181782\n",
      "Epoch 97, loss: 2.179036\n",
      "Epoch 98, loss: 2.225775\n",
      "Epoch 99, loss: 2.205462\n",
      "Epoch 100, loss: 2.210903\n",
      "Epoch 101, loss: 2.208454\n",
      "Epoch 102, loss: 2.192520\n",
      "Epoch 103, loss: 2.210860\n",
      "Epoch 104, loss: 2.222238\n",
      "Epoch 105, loss: 2.197657\n",
      "Epoch 106, loss: 2.191084\n",
      "Epoch 107, loss: 2.232780\n",
      "Epoch 108, loss: 2.183149\n",
      "Epoch 109, loss: 2.206425\n",
      "Epoch 110, loss: 2.188914\n",
      "Epoch 111, loss: 2.200897\n",
      "Epoch 112, loss: 2.216575\n",
      "Epoch 113, loss: 2.172899\n",
      "Epoch 114, loss: 2.190103\n",
      "Epoch 115, loss: 2.209520\n",
      "Epoch 116, loss: 2.173627\n",
      "Epoch 117, loss: 2.194671\n",
      "Epoch 118, loss: 2.201569\n",
      "Epoch 119, loss: 2.192528\n",
      "Epoch 120, loss: 2.215627\n",
      "Epoch 121, loss: 2.193871\n",
      "Epoch 122, loss: 2.183480\n",
      "Epoch 123, loss: 2.163271\n",
      "Epoch 124, loss: 2.206620\n",
      "Epoch 125, loss: 2.205797\n",
      "Epoch 126, loss: 2.200251\n",
      "Epoch 127, loss: 2.200897\n",
      "Epoch 128, loss: 2.207052\n",
      "Epoch 129, loss: 2.180102\n",
      "Epoch 130, loss: 2.237317\n",
      "Epoch 131, loss: 2.172290\n",
      "Epoch 132, loss: 2.190133\n",
      "Epoch 133, loss: 2.178523\n",
      "Epoch 134, loss: 2.179394\n",
      "Epoch 135, loss: 2.183154\n",
      "Epoch 136, loss: 2.201790\n",
      "Epoch 137, loss: 2.198015\n",
      "Epoch 138, loss: 2.171627\n",
      "Epoch 139, loss: 2.213755\n",
      "Epoch 140, loss: 2.190078\n",
      "Epoch 141, loss: 2.189239\n",
      "Epoch 142, loss: 2.216309\n",
      "Epoch 143, loss: 2.192095\n",
      "Epoch 144, loss: 2.223568\n",
      "Epoch 145, loss: 2.166172\n",
      "Epoch 146, loss: 2.194272\n",
      "Epoch 147, loss: 2.185894\n",
      "Epoch 148, loss: 2.213173\n",
      "Epoch 149, loss: 2.165760\n",
      "Epoch 150, loss: 2.183365\n",
      "Epoch 151, loss: 2.186363\n",
      "Epoch 152, loss: 2.211172\n",
      "Epoch 153, loss: 2.187503\n",
      "Epoch 154, loss: 2.180002\n",
      "Epoch 155, loss: 2.188321\n",
      "Epoch 156, loss: 2.214144\n",
      "Epoch 157, loss: 2.167450\n",
      "Epoch 158, loss: 2.163339\n",
      "Epoch 159, loss: 2.165040\n",
      "Epoch 160, loss: 2.238077\n",
      "Epoch 161, loss: 2.189519\n",
      "Epoch 162, loss: 2.158611\n",
      "Epoch 163, loss: 2.159575\n",
      "Epoch 164, loss: 2.184274\n",
      "Epoch 165, loss: 2.157848\n",
      "Epoch 166, loss: 2.166875\n",
      "Epoch 167, loss: 2.190067\n",
      "Epoch 168, loss: 2.177938\n",
      "Epoch 169, loss: 2.160535\n",
      "Epoch 170, loss: 2.214149\n",
      "Epoch 171, loss: 2.211909\n",
      "Epoch 172, loss: 2.148033\n",
      "Epoch 173, loss: 2.151645\n",
      "Epoch 174, loss: 2.163343\n",
      "Epoch 175, loss: 2.151174\n",
      "Epoch 176, loss: 2.169896\n",
      "Epoch 177, loss: 2.214240\n",
      "Epoch 178, loss: 2.160619\n",
      "Epoch 179, loss: 2.205934\n",
      "Epoch 180, loss: 2.200695\n",
      "Epoch 181, loss: 2.172314\n",
      "Epoch 182, loss: 2.137901\n",
      "Epoch 183, loss: 2.178918\n",
      "Epoch 184, loss: 2.186429\n",
      "Epoch 185, loss: 2.174452\n",
      "Epoch 186, loss: 2.179489\n",
      "Epoch 187, loss: 2.150291\n",
      "Epoch 188, loss: 2.118176\n",
      "Epoch 189, loss: 2.147589\n",
      "Epoch 190, loss: 2.177616\n",
      "Epoch 191, loss: 2.171964\n",
      "Epoch 192, loss: 2.197062\n",
      "Epoch 193, loss: 2.201037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 2.183278\n",
      "Epoch 195, loss: 2.163465\n",
      "Epoch 196, loss: 2.177384\n",
      "Epoch 197, loss: 2.180790\n",
      "Epoch 198, loss: 2.196982\n",
      "Epoch 199, loss: 2.175645\n",
      "Epoch 0, loss: 2.302753\n",
      "Epoch 1, loss: 2.305904\n",
      "Epoch 2, loss: 2.293694\n",
      "Epoch 3, loss: 2.289111\n",
      "Epoch 4, loss: 2.306520\n",
      "Epoch 5, loss: 2.297266\n",
      "Epoch 6, loss: 2.285783\n",
      "Epoch 7, loss: 2.291607\n",
      "Epoch 8, loss: 2.302968\n",
      "Epoch 9, loss: 2.292214\n",
      "Epoch 10, loss: 2.289902\n",
      "Epoch 11, loss: 2.280526\n",
      "Epoch 12, loss: 2.289543\n",
      "Epoch 13, loss: 2.283447\n",
      "Epoch 14, loss: 2.288812\n",
      "Epoch 15, loss: 2.280403\n",
      "Epoch 16, loss: 2.283279\n",
      "Epoch 17, loss: 2.282033\n",
      "Epoch 18, loss: 2.272413\n",
      "Epoch 19, loss: 2.267383\n",
      "Epoch 20, loss: 2.284326\n",
      "Epoch 21, loss: 2.262904\n",
      "Epoch 22, loss: 2.277935\n",
      "Epoch 23, loss: 2.270217\n",
      "Epoch 24, loss: 2.277593\n",
      "Epoch 25, loss: 2.267221\n",
      "Epoch 26, loss: 2.261771\n",
      "Epoch 27, loss: 2.266340\n",
      "Epoch 28, loss: 2.258409\n",
      "Epoch 29, loss: 2.284440\n",
      "Epoch 30, loss: 2.261700\n",
      "Epoch 31, loss: 2.284970\n",
      "Epoch 32, loss: 2.255402\n",
      "Epoch 33, loss: 2.267280\n",
      "Epoch 34, loss: 2.255437\n",
      "Epoch 35, loss: 2.273413\n",
      "Epoch 36, loss: 2.249946\n",
      "Epoch 37, loss: 2.227818\n",
      "Epoch 38, loss: 2.245567\n",
      "Epoch 39, loss: 2.257984\n",
      "Epoch 40, loss: 2.256860\n",
      "Epoch 41, loss: 2.252742\n",
      "Epoch 42, loss: 2.243660\n",
      "Epoch 43, loss: 2.253645\n",
      "Epoch 44, loss: 2.264064\n",
      "Epoch 45, loss: 2.251113\n",
      "Epoch 46, loss: 2.252681\n",
      "Epoch 47, loss: 2.245608\n",
      "Epoch 48, loss: 2.236762\n",
      "Epoch 49, loss: 2.212300\n",
      "Epoch 50, loss: 2.252741\n",
      "Epoch 51, loss: 2.245561\n",
      "Epoch 52, loss: 2.223485\n",
      "Epoch 53, loss: 2.221606\n",
      "Epoch 54, loss: 2.228930\n",
      "Epoch 55, loss: 2.238559\n",
      "Epoch 56, loss: 2.226732\n",
      "Epoch 57, loss: 2.229170\n",
      "Epoch 58, loss: 2.220709\n",
      "Epoch 59, loss: 2.237729\n",
      "Epoch 60, loss: 2.233098\n",
      "Epoch 61, loss: 2.229159\n",
      "Epoch 62, loss: 2.242208\n",
      "Epoch 63, loss: 2.237585\n",
      "Epoch 64, loss: 2.232623\n",
      "Epoch 65, loss: 2.237002\n",
      "Epoch 66, loss: 2.239133\n",
      "Epoch 67, loss: 2.195773\n",
      "Epoch 68, loss: 2.216857\n",
      "Epoch 69, loss: 2.206214\n",
      "Epoch 70, loss: 2.210640\n",
      "Epoch 71, loss: 2.219008\n",
      "Epoch 72, loss: 2.216115\n",
      "Epoch 73, loss: 2.195112\n",
      "Epoch 74, loss: 2.212398\n",
      "Epoch 75, loss: 2.244020\n",
      "Epoch 76, loss: 2.214940\n",
      "Epoch 77, loss: 2.235669\n",
      "Epoch 78, loss: 2.219302\n",
      "Epoch 79, loss: 2.217918\n",
      "Epoch 80, loss: 2.230249\n",
      "Epoch 81, loss: 2.223545\n",
      "Epoch 82, loss: 2.183723\n",
      "Epoch 83, loss: 2.224033\n",
      "Epoch 84, loss: 2.236159\n",
      "Epoch 85, loss: 2.229219\n",
      "Epoch 86, loss: 2.220959\n",
      "Epoch 87, loss: 2.208784\n",
      "Epoch 88, loss: 2.201540\n",
      "Epoch 89, loss: 2.231931\n",
      "Epoch 90, loss: 2.206719\n",
      "Epoch 91, loss: 2.179443\n",
      "Epoch 92, loss: 2.211740\n",
      "Epoch 93, loss: 2.213221\n",
      "Epoch 94, loss: 2.192867\n",
      "Epoch 95, loss: 2.211223\n",
      "Epoch 96, loss: 2.196388\n",
      "Epoch 97, loss: 2.216460\n",
      "Epoch 98, loss: 2.190659\n",
      "Epoch 99, loss: 2.204241\n",
      "Epoch 100, loss: 2.218138\n",
      "Epoch 101, loss: 2.212085\n",
      "Epoch 102, loss: 2.196607\n",
      "Epoch 103, loss: 2.208059\n",
      "Epoch 104, loss: 2.194287\n",
      "Epoch 105, loss: 2.204062\n",
      "Epoch 106, loss: 2.208442\n",
      "Epoch 107, loss: 2.194518\n",
      "Epoch 108, loss: 2.201916\n",
      "Epoch 109, loss: 2.195922\n",
      "Epoch 110, loss: 2.190396\n",
      "Epoch 111, loss: 2.196814\n",
      "Epoch 112, loss: 2.186894\n",
      "Epoch 113, loss: 2.220461\n",
      "Epoch 114, loss: 2.170563\n",
      "Epoch 115, loss: 2.200428\n",
      "Epoch 116, loss: 2.224400\n",
      "Epoch 117, loss: 2.216764\n",
      "Epoch 118, loss: 2.201525\n",
      "Epoch 119, loss: 2.178667\n",
      "Epoch 120, loss: 2.216459\n",
      "Epoch 121, loss: 2.204651\n",
      "Epoch 122, loss: 2.175143\n",
      "Epoch 123, loss: 2.205379\n",
      "Epoch 124, loss: 2.226688\n",
      "Epoch 125, loss: 2.217571\n",
      "Epoch 126, loss: 2.236092\n",
      "Epoch 127, loss: 2.199110\n",
      "Epoch 128, loss: 2.192270\n",
      "Epoch 129, loss: 2.224668\n",
      "Epoch 130, loss: 2.207568\n",
      "Epoch 131, loss: 2.204661\n",
      "Epoch 132, loss: 2.188658\n",
      "Epoch 133, loss: 2.163686\n",
      "Epoch 134, loss: 2.190158\n",
      "Epoch 135, loss: 2.195946\n",
      "Epoch 136, loss: 2.174203\n",
      "Epoch 137, loss: 2.172287\n",
      "Epoch 138, loss: 2.221796\n",
      "Epoch 139, loss: 2.178353\n",
      "Epoch 140, loss: 2.179959\n",
      "Epoch 141, loss: 2.171524\n",
      "Epoch 142, loss: 2.188791\n",
      "Epoch 143, loss: 2.194419\n",
      "Epoch 144, loss: 2.219327\n",
      "Epoch 145, loss: 2.195470\n",
      "Epoch 146, loss: 2.211439\n",
      "Epoch 147, loss: 2.149819\n",
      "Epoch 148, loss: 2.195577\n",
      "Epoch 149, loss: 2.215114\n",
      "Epoch 150, loss: 2.176276\n",
      "Epoch 151, loss: 2.216756\n",
      "Epoch 152, loss: 2.183063\n",
      "Epoch 153, loss: 2.183402\n",
      "Epoch 154, loss: 2.201151\n",
      "Epoch 155, loss: 2.214281\n",
      "Epoch 156, loss: 2.175475\n",
      "Epoch 157, loss: 2.151210\n",
      "Epoch 158, loss: 2.150737\n",
      "Epoch 159, loss: 2.159669\n",
      "Epoch 160, loss: 2.203428\n",
      "Epoch 161, loss: 2.196244\n",
      "Epoch 162, loss: 2.200839\n",
      "Epoch 163, loss: 2.150412\n",
      "Epoch 164, loss: 2.171779\n",
      "Epoch 165, loss: 2.177910\n",
      "Epoch 166, loss: 2.205187\n",
      "Epoch 167, loss: 2.191076\n",
      "Epoch 168, loss: 2.152882\n",
      "Epoch 169, loss: 2.194267\n",
      "Epoch 170, loss: 2.162909\n",
      "Epoch 171, loss: 2.168417\n",
      "Epoch 172, loss: 2.184295\n",
      "Epoch 173, loss: 2.186607\n",
      "Epoch 174, loss: 2.161423\n",
      "Epoch 175, loss: 2.183616\n",
      "Epoch 176, loss: 2.168615\n",
      "Epoch 177, loss: 2.178332\n",
      "Epoch 178, loss: 2.177736\n",
      "Epoch 179, loss: 2.140747\n",
      "Epoch 180, loss: 2.162155\n",
      "Epoch 181, loss: 2.148945\n",
      "Epoch 182, loss: 2.125104\n",
      "Epoch 183, loss: 2.191052\n",
      "Epoch 184, loss: 2.173537\n",
      "Epoch 185, loss: 2.166234\n",
      "Epoch 186, loss: 2.145345\n",
      "Epoch 187, loss: 2.168133\n",
      "Epoch 188, loss: 2.216383\n",
      "Epoch 189, loss: 2.230789\n",
      "Epoch 190, loss: 2.138738\n",
      "Epoch 191, loss: 2.139809\n",
      "Epoch 192, loss: 2.205687\n",
      "Epoch 193, loss: 2.157133\n",
      "Epoch 194, loss: 2.138396\n",
      "Epoch 195, loss: 2.156023\n",
      "Epoch 196, loss: 2.176095\n",
      "Epoch 197, loss: 2.163509\n",
      "Epoch 198, loss: 2.181622\n",
      "Epoch 199, loss: 2.192271\n",
      "Epoch 0, loss: 2.302288\n",
      "Epoch 1, loss: 2.308552\n",
      "Epoch 2, loss: 2.301455\n",
      "Epoch 3, loss: 2.304347\n",
      "Epoch 4, loss: 2.312053\n",
      "Epoch 5, loss: 2.299706\n",
      "Epoch 6, loss: 2.286559\n",
      "Epoch 7, loss: 2.298000\n",
      "Epoch 8, loss: 2.293384\n",
      "Epoch 9, loss: 2.294446\n",
      "Epoch 10, loss: 2.294698\n",
      "Epoch 11, loss: 2.290153\n",
      "Epoch 12, loss: 2.272714\n",
      "Epoch 13, loss: 2.289906\n",
      "Epoch 14, loss: 2.272986\n",
      "Epoch 15, loss: 2.276997\n",
      "Epoch 16, loss: 2.278273\n",
      "Epoch 17, loss: 2.273700\n",
      "Epoch 18, loss: 2.280210\n",
      "Epoch 19, loss: 2.276759\n",
      "Epoch 20, loss: 2.265601\n",
      "Epoch 21, loss: 2.291746\n",
      "Epoch 22, loss: 2.278256\n",
      "Epoch 23, loss: 2.249551\n",
      "Epoch 24, loss: 2.279775\n",
      "Epoch 25, loss: 2.279422\n",
      "Epoch 26, loss: 2.261460\n",
      "Epoch 27, loss: 2.254597\n",
      "Epoch 28, loss: 2.261292\n",
      "Epoch 29, loss: 2.262132\n",
      "Epoch 30, loss: 2.245535\n",
      "Epoch 31, loss: 2.260347\n",
      "Epoch 32, loss: 2.259521\n",
      "Epoch 33, loss: 2.262384\n",
      "Epoch 34, loss: 2.257371\n",
      "Epoch 35, loss: 2.271424\n",
      "Epoch 36, loss: 2.247673\n",
      "Epoch 37, loss: 2.245679\n",
      "Epoch 38, loss: 2.263542\n",
      "Epoch 39, loss: 2.250104\n",
      "Epoch 40, loss: 2.253629\n",
      "Epoch 41, loss: 2.260610\n",
      "Epoch 42, loss: 2.253540\n",
      "Epoch 43, loss: 2.246841\n",
      "Epoch 44, loss: 2.227647\n",
      "Epoch 45, loss: 2.265306\n",
      "Epoch 46, loss: 2.252548\n",
      "Epoch 47, loss: 2.236019\n",
      "Epoch 48, loss: 2.248142\n",
      "Epoch 49, loss: 2.272127\n",
      "Epoch 50, loss: 2.228354\n",
      "Epoch 51, loss: 2.226967\n",
      "Epoch 52, loss: 2.240426\n",
      "Epoch 53, loss: 2.248354\n",
      "Epoch 54, loss: 2.248557\n",
      "Epoch 55, loss: 2.238626\n",
      "Epoch 56, loss: 2.213698\n",
      "Epoch 57, loss: 2.216155\n",
      "Epoch 58, loss: 2.241483\n",
      "Epoch 59, loss: 2.220249\n",
      "Epoch 60, loss: 2.239191\n",
      "Epoch 61, loss: 2.242558\n",
      "Epoch 62, loss: 2.230968\n",
      "Epoch 63, loss: 2.231008\n",
      "Epoch 64, loss: 2.222111\n",
      "Epoch 65, loss: 2.226666\n",
      "Epoch 66, loss: 2.226070\n",
      "Epoch 67, loss: 2.207825\n",
      "Epoch 68, loss: 2.232899\n",
      "Epoch 69, loss: 2.242527\n",
      "Epoch 70, loss: 2.220081\n",
      "Epoch 71, loss: 2.195055\n",
      "Epoch 72, loss: 2.218605\n",
      "Epoch 73, loss: 2.251926\n",
      "Epoch 74, loss: 2.231388\n",
      "Epoch 75, loss: 2.221176\n",
      "Epoch 76, loss: 2.239078\n",
      "Epoch 77, loss: 2.215927\n",
      "Epoch 78, loss: 2.181278\n",
      "Epoch 79, loss: 2.211296\n",
      "Epoch 80, loss: 2.207411\n",
      "Epoch 81, loss: 2.212458\n",
      "Epoch 82, loss: 2.217574\n",
      "Epoch 83, loss: 2.212390\n",
      "Epoch 84, loss: 2.198975\n",
      "Epoch 85, loss: 2.229211\n",
      "Epoch 86, loss: 2.236142\n",
      "Epoch 87, loss: 2.188839\n",
      "Epoch 88, loss: 2.232074\n",
      "Epoch 89, loss: 2.211711\n",
      "Epoch 90, loss: 2.200282\n",
      "Epoch 91, loss: 2.216563\n",
      "Epoch 92, loss: 2.212990\n",
      "Epoch 93, loss: 2.188756\n",
      "Epoch 94, loss: 2.186784\n",
      "Epoch 95, loss: 2.203179\n",
      "Epoch 96, loss: 2.204780\n",
      "Epoch 97, loss: 2.213960\n",
      "Epoch 98, loss: 2.232108\n",
      "Epoch 99, loss: 2.178154\n",
      "Epoch 100, loss: 2.207583\n",
      "Epoch 101, loss: 2.193499\n",
      "Epoch 102, loss: 2.197799\n",
      "Epoch 103, loss: 2.207399\n",
      "Epoch 104, loss: 2.221683\n",
      "Epoch 105, loss: 2.194811\n",
      "Epoch 106, loss: 2.199118\n",
      "Epoch 107, loss: 2.201572\n",
      "Epoch 108, loss: 2.233056\n",
      "Epoch 109, loss: 2.192399\n",
      "Epoch 110, loss: 2.170183\n",
      "Epoch 111, loss: 2.233416\n",
      "Epoch 112, loss: 2.193408\n",
      "Epoch 113, loss: 2.188264\n",
      "Epoch 114, loss: 2.196959\n",
      "Epoch 115, loss: 2.181373\n",
      "Epoch 116, loss: 2.176232\n",
      "Epoch 117, loss: 2.220807\n",
      "Epoch 118, loss: 2.219778\n",
      "Epoch 119, loss: 2.186701\n",
      "Epoch 120, loss: 2.195281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, loss: 2.184154\n",
      "Epoch 122, loss: 2.197958\n",
      "Epoch 123, loss: 2.212431\n",
      "Epoch 124, loss: 2.172549\n",
      "Epoch 125, loss: 2.211260\n",
      "Epoch 126, loss: 2.179814\n",
      "Epoch 127, loss: 2.178342\n",
      "Epoch 128, loss: 2.223336\n",
      "Epoch 129, loss: 2.180245\n",
      "Epoch 130, loss: 2.191674\n",
      "Epoch 131, loss: 2.184475\n",
      "Epoch 132, loss: 2.184526\n",
      "Epoch 133, loss: 2.195370\n",
      "Epoch 134, loss: 2.170156\n",
      "Epoch 135, loss: 2.188318\n",
      "Epoch 136, loss: 2.181478\n",
      "Epoch 137, loss: 2.180001\n",
      "Epoch 138, loss: 2.188483\n",
      "Epoch 139, loss: 2.194133\n",
      "Epoch 140, loss: 2.190104\n",
      "Epoch 141, loss: 2.213876\n",
      "Epoch 142, loss: 2.221194\n",
      "Epoch 143, loss: 2.158048\n",
      "Epoch 144, loss: 2.181009\n",
      "Epoch 145, loss: 2.191424\n",
      "Epoch 146, loss: 2.175923\n",
      "Epoch 147, loss: 2.199249\n",
      "Epoch 148, loss: 2.186298\n",
      "Epoch 149, loss: 2.241982\n",
      "Epoch 150, loss: 2.188559\n",
      "Epoch 151, loss: 2.186122\n",
      "Epoch 152, loss: 2.175343\n",
      "Epoch 153, loss: 2.186644\n",
      "Epoch 154, loss: 2.142775\n",
      "Epoch 155, loss: 2.178374\n",
      "Epoch 156, loss: 2.204797\n",
      "Epoch 157, loss: 2.181625\n",
      "Epoch 158, loss: 2.173283\n",
      "Epoch 159, loss: 2.202861\n",
      "Epoch 160, loss: 2.181012\n",
      "Epoch 161, loss: 2.192760\n",
      "Epoch 162, loss: 2.158259\n",
      "Epoch 163, loss: 2.182257\n",
      "Epoch 164, loss: 2.178434\n",
      "Epoch 165, loss: 2.136655\n",
      "Epoch 166, loss: 2.162759\n",
      "Epoch 167, loss: 2.152456\n",
      "Epoch 168, loss: 2.175465\n",
      "Epoch 169, loss: 2.164048\n",
      "Epoch 170, loss: 2.178221\n",
      "Epoch 171, loss: 2.187233\n",
      "Epoch 172, loss: 2.185043\n",
      "Epoch 173, loss: 2.118570\n",
      "Epoch 174, loss: 2.132295\n",
      "Epoch 175, loss: 2.164873\n",
      "Epoch 176, loss: 2.203404\n",
      "Epoch 177, loss: 2.226524\n",
      "Epoch 178, loss: 2.161514\n",
      "Epoch 179, loss: 2.152293\n",
      "Epoch 180, loss: 2.172788\n",
      "Epoch 181, loss: 2.156128\n",
      "Epoch 182, loss: 2.187824\n",
      "Epoch 183, loss: 2.205987\n",
      "Epoch 184, loss: 2.181558\n",
      "Epoch 185, loss: 2.159795\n",
      "Epoch 186, loss: 2.154304\n",
      "Epoch 187, loss: 2.158878\n",
      "Epoch 188, loss: 2.128033\n",
      "Epoch 189, loss: 2.176548\n",
      "Epoch 190, loss: 2.175243\n",
      "Epoch 191, loss: 2.185590\n",
      "Epoch 192, loss: 2.188363\n",
      "Epoch 193, loss: 2.163067\n",
      "Epoch 194, loss: 2.204425\n",
      "Epoch 195, loss: 2.182401\n",
      "Epoch 196, loss: 2.150333\n",
      "Epoch 197, loss: 2.138175\n",
      "Epoch 198, loss: 2.198339\n",
      "Epoch 199, loss: 2.143049\n",
      "Epoch 0, loss: 2.302752\n",
      "Epoch 1, loss: 2.301153\n",
      "Epoch 2, loss: 2.304225\n",
      "Epoch 3, loss: 2.300604\n",
      "Epoch 4, loss: 2.300213\n",
      "Epoch 5, loss: 2.302063\n",
      "Epoch 6, loss: 2.302983\n",
      "Epoch 7, loss: 2.303997\n",
      "Epoch 8, loss: 2.301445\n",
      "Epoch 9, loss: 2.303911\n",
      "Epoch 10, loss: 2.301948\n",
      "Epoch 11, loss: 2.300593\n",
      "Epoch 12, loss: 2.301260\n",
      "Epoch 13, loss: 2.300262\n",
      "Epoch 14, loss: 2.299149\n",
      "Epoch 15, loss: 2.299873\n",
      "Epoch 16, loss: 2.296860\n",
      "Epoch 17, loss: 2.301976\n",
      "Epoch 18, loss: 2.299258\n",
      "Epoch 19, loss: 2.301036\n",
      "Epoch 20, loss: 2.300321\n",
      "Epoch 21, loss: 2.299474\n",
      "Epoch 22, loss: 2.302435\n",
      "Epoch 23, loss: 2.298264\n",
      "Epoch 24, loss: 2.300217\n",
      "Epoch 25, loss: 2.301739\n",
      "Epoch 26, loss: 2.299868\n",
      "Epoch 27, loss: 2.297478\n",
      "Epoch 28, loss: 2.297202\n",
      "Epoch 29, loss: 2.298940\n",
      "Epoch 30, loss: 2.297168\n",
      "Epoch 31, loss: 2.299095\n",
      "Epoch 32, loss: 2.299181\n",
      "Epoch 33, loss: 2.297835\n",
      "Epoch 34, loss: 2.300781\n",
      "Epoch 35, loss: 2.298433\n",
      "Epoch 36, loss: 2.298150\n",
      "Epoch 37, loss: 2.296865\n",
      "Epoch 38, loss: 2.298658\n",
      "Epoch 39, loss: 2.294869\n",
      "Epoch 40, loss: 2.300668\n",
      "Epoch 41, loss: 2.297174\n",
      "Epoch 42, loss: 2.296697\n",
      "Epoch 43, loss: 2.293238\n",
      "Epoch 44, loss: 2.295927\n",
      "Epoch 45, loss: 2.294873\n",
      "Epoch 46, loss: 2.296077\n",
      "Epoch 47, loss: 2.295899\n",
      "Epoch 48, loss: 2.295835\n",
      "Epoch 49, loss: 2.293375\n",
      "Epoch 50, loss: 2.294820\n",
      "Epoch 51, loss: 2.296466\n",
      "Epoch 52, loss: 2.298665\n",
      "Epoch 53, loss: 2.298365\n",
      "Epoch 54, loss: 2.298532\n",
      "Epoch 55, loss: 2.294044\n",
      "Epoch 56, loss: 2.295201\n",
      "Epoch 57, loss: 2.292038\n",
      "Epoch 58, loss: 2.294476\n",
      "Epoch 59, loss: 2.293204\n",
      "Epoch 60, loss: 2.295401\n",
      "Epoch 61, loss: 2.296088\n",
      "Epoch 62, loss: 2.289572\n",
      "Epoch 63, loss: 2.290299\n",
      "Epoch 64, loss: 2.294463\n",
      "Epoch 65, loss: 2.290680\n",
      "Epoch 66, loss: 2.290653\n",
      "Epoch 67, loss: 2.289067\n",
      "Epoch 68, loss: 2.292515\n",
      "Epoch 69, loss: 2.295428\n",
      "Epoch 70, loss: 2.287576\n",
      "Epoch 71, loss: 2.288158\n",
      "Epoch 72, loss: 2.289574\n",
      "Epoch 73, loss: 2.291059\n",
      "Epoch 74, loss: 2.294709\n",
      "Epoch 75, loss: 2.295542\n",
      "Epoch 76, loss: 2.293074\n",
      "Epoch 77, loss: 2.291203\n",
      "Epoch 78, loss: 2.292162\n",
      "Epoch 79, loss: 2.288095\n",
      "Epoch 80, loss: 2.290500\n",
      "Epoch 81, loss: 2.292653\n",
      "Epoch 82, loss: 2.292314\n",
      "Epoch 83, loss: 2.289931\n",
      "Epoch 84, loss: 2.294372\n",
      "Epoch 85, loss: 2.289305\n",
      "Epoch 86, loss: 2.291658\n",
      "Epoch 87, loss: 2.286376\n",
      "Epoch 88, loss: 2.289028\n",
      "Epoch 89, loss: 2.281764\n",
      "Epoch 90, loss: 2.291263\n",
      "Epoch 91, loss: 2.288737\n",
      "Epoch 92, loss: 2.290054\n",
      "Epoch 93, loss: 2.285077\n",
      "Epoch 94, loss: 2.288468\n",
      "Epoch 95, loss: 2.287926\n",
      "Epoch 96, loss: 2.289561\n",
      "Epoch 97, loss: 2.290931\n",
      "Epoch 98, loss: 2.288976\n",
      "Epoch 99, loss: 2.287010\n",
      "Epoch 100, loss: 2.288667\n",
      "Epoch 101, loss: 2.288563\n",
      "Epoch 102, loss: 2.283328\n",
      "Epoch 103, loss: 2.290156\n",
      "Epoch 104, loss: 2.285581\n",
      "Epoch 105, loss: 2.286849\n",
      "Epoch 106, loss: 2.288008\n",
      "Epoch 107, loss: 2.286567\n",
      "Epoch 108, loss: 2.282615\n",
      "Epoch 109, loss: 2.291222\n",
      "Epoch 110, loss: 2.286889\n",
      "Epoch 111, loss: 2.276483\n",
      "Epoch 112, loss: 2.287048\n",
      "Epoch 113, loss: 2.288922\n",
      "Epoch 114, loss: 2.287108\n",
      "Epoch 115, loss: 2.290949\n",
      "Epoch 116, loss: 2.283610\n",
      "Epoch 117, loss: 2.283832\n",
      "Epoch 118, loss: 2.291136\n",
      "Epoch 119, loss: 2.289191\n",
      "Epoch 120, loss: 2.282206\n",
      "Epoch 121, loss: 2.284221\n",
      "Epoch 122, loss: 2.279543\n",
      "Epoch 123, loss: 2.280202\n",
      "Epoch 124, loss: 2.285098\n",
      "Epoch 125, loss: 2.290248\n",
      "Epoch 126, loss: 2.282607\n",
      "Epoch 127, loss: 2.281107\n",
      "Epoch 128, loss: 2.289696\n",
      "Epoch 129, loss: 2.286017\n",
      "Epoch 130, loss: 2.284642\n",
      "Epoch 131, loss: 2.284920\n",
      "Epoch 132, loss: 2.281607\n",
      "Epoch 133, loss: 2.285396\n",
      "Epoch 134, loss: 2.281116\n",
      "Epoch 135, loss: 2.284598\n",
      "Epoch 136, loss: 2.279284\n",
      "Epoch 137, loss: 2.283071\n",
      "Epoch 138, loss: 2.284590\n",
      "Epoch 139, loss: 2.278833\n",
      "Epoch 140, loss: 2.286563\n",
      "Epoch 141, loss: 2.283065\n",
      "Epoch 142, loss: 2.279620\n",
      "Epoch 143, loss: 2.280405\n",
      "Epoch 144, loss: 2.281367\n",
      "Epoch 145, loss: 2.276010\n",
      "Epoch 146, loss: 2.280286\n",
      "Epoch 147, loss: 2.281252\n",
      "Epoch 148, loss: 2.277769\n",
      "Epoch 149, loss: 2.276418\n",
      "Epoch 150, loss: 2.281023\n",
      "Epoch 151, loss: 2.282658\n",
      "Epoch 152, loss: 2.281514\n",
      "Epoch 153, loss: 2.282740\n",
      "Epoch 154, loss: 2.278555\n",
      "Epoch 155, loss: 2.280301\n",
      "Epoch 156, loss: 2.277615\n",
      "Epoch 157, loss: 2.280370\n",
      "Epoch 158, loss: 2.281633\n",
      "Epoch 159, loss: 2.276407\n",
      "Epoch 160, loss: 2.273213\n",
      "Epoch 161, loss: 2.279692\n",
      "Epoch 162, loss: 2.287355\n",
      "Epoch 163, loss: 2.281790\n",
      "Epoch 164, loss: 2.278701\n",
      "Epoch 165, loss: 2.278687\n",
      "Epoch 166, loss: 2.276154\n",
      "Epoch 167, loss: 2.280215\n",
      "Epoch 168, loss: 2.271325\n",
      "Epoch 169, loss: 2.277417\n",
      "Epoch 170, loss: 2.283110\n",
      "Epoch 171, loss: 2.279266\n",
      "Epoch 172, loss: 2.275326\n",
      "Epoch 173, loss: 2.275465\n",
      "Epoch 174, loss: 2.274580\n",
      "Epoch 175, loss: 2.275443\n",
      "Epoch 176, loss: 2.275221\n",
      "Epoch 177, loss: 2.283173\n",
      "Epoch 178, loss: 2.288038\n",
      "Epoch 179, loss: 2.271212\n",
      "Epoch 180, loss: 2.278481\n",
      "Epoch 181, loss: 2.265479\n",
      "Epoch 182, loss: 2.272810\n",
      "Epoch 183, loss: 2.265940\n",
      "Epoch 184, loss: 2.277582\n",
      "Epoch 185, loss: 2.275710\n",
      "Epoch 186, loss: 2.283468\n",
      "Epoch 187, loss: 2.279960\n",
      "Epoch 188, loss: 2.275935\n",
      "Epoch 189, loss: 2.282391\n",
      "Epoch 190, loss: 2.274554\n",
      "Epoch 191, loss: 2.264184\n",
      "Epoch 192, loss: 2.270359\n",
      "Epoch 193, loss: 2.278012\n",
      "Epoch 194, loss: 2.286788\n",
      "Epoch 195, loss: 2.276035\n",
      "Epoch 196, loss: 2.281615\n",
      "Epoch 197, loss: 2.270346\n",
      "Epoch 198, loss: 2.274156\n",
      "Epoch 199, loss: 2.267990\n",
      "Epoch 0, loss: 2.302248\n",
      "Epoch 1, loss: 2.302895\n",
      "Epoch 2, loss: 2.303643\n",
      "Epoch 3, loss: 2.302179\n",
      "Epoch 4, loss: 2.301335\n",
      "Epoch 5, loss: 2.301278\n",
      "Epoch 6, loss: 2.302033\n",
      "Epoch 7, loss: 2.302300\n",
      "Epoch 8, loss: 2.302524\n",
      "Epoch 9, loss: 2.301529\n",
      "Epoch 10, loss: 2.302947\n",
      "Epoch 11, loss: 2.300693\n",
      "Epoch 12, loss: 2.299240\n",
      "Epoch 13, loss: 2.300282\n",
      "Epoch 14, loss: 2.298120\n",
      "Epoch 15, loss: 2.299775\n",
      "Epoch 16, loss: 2.303206\n",
      "Epoch 17, loss: 2.297856\n",
      "Epoch 18, loss: 2.300308\n",
      "Epoch 19, loss: 2.300059\n",
      "Epoch 20, loss: 2.298926\n",
      "Epoch 21, loss: 2.303097\n",
      "Epoch 22, loss: 2.298833\n",
      "Epoch 23, loss: 2.301037\n",
      "Epoch 24, loss: 2.297945\n",
      "Epoch 25, loss: 2.298785\n",
      "Epoch 26, loss: 2.299677\n",
      "Epoch 27, loss: 2.298105\n",
      "Epoch 28, loss: 2.299878\n",
      "Epoch 29, loss: 2.298766\n",
      "Epoch 30, loss: 2.299345\n",
      "Epoch 31, loss: 2.298825\n",
      "Epoch 32, loss: 2.296092\n",
      "Epoch 33, loss: 2.297970\n",
      "Epoch 34, loss: 2.298907\n",
      "Epoch 35, loss: 2.296690\n",
      "Epoch 36, loss: 2.298476\n",
      "Epoch 37, loss: 2.297679\n",
      "Epoch 38, loss: 2.295338\n",
      "Epoch 39, loss: 2.299132\n",
      "Epoch 40, loss: 2.294877\n",
      "Epoch 41, loss: 2.292664\n",
      "Epoch 42, loss: 2.297328\n",
      "Epoch 43, loss: 2.291894\n",
      "Epoch 44, loss: 2.299383\n",
      "Epoch 45, loss: 2.295898\n",
      "Epoch 46, loss: 2.297620\n",
      "Epoch 47, loss: 2.296569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, loss: 2.295912\n",
      "Epoch 49, loss: 2.296942\n",
      "Epoch 50, loss: 2.296067\n",
      "Epoch 51, loss: 2.291137\n",
      "Epoch 52, loss: 2.293987\n",
      "Epoch 53, loss: 2.294188\n",
      "Epoch 54, loss: 2.296392\n",
      "Epoch 55, loss: 2.292417\n",
      "Epoch 56, loss: 2.293059\n",
      "Epoch 57, loss: 2.290053\n",
      "Epoch 58, loss: 2.295289\n",
      "Epoch 59, loss: 2.291857\n",
      "Epoch 60, loss: 2.299815\n",
      "Epoch 61, loss: 2.295452\n",
      "Epoch 62, loss: 2.290345\n",
      "Epoch 63, loss: 2.294317\n",
      "Epoch 64, loss: 2.292697\n",
      "Epoch 65, loss: 2.290655\n",
      "Epoch 66, loss: 2.294681\n",
      "Epoch 67, loss: 2.293144\n",
      "Epoch 68, loss: 2.293817\n",
      "Epoch 69, loss: 2.292059\n",
      "Epoch 70, loss: 2.291816\n",
      "Epoch 71, loss: 2.292749\n",
      "Epoch 72, loss: 2.295168\n",
      "Epoch 73, loss: 2.294596\n",
      "Epoch 74, loss: 2.294187\n",
      "Epoch 75, loss: 2.293624\n",
      "Epoch 76, loss: 2.285492\n",
      "Epoch 77, loss: 2.287599\n",
      "Epoch 78, loss: 2.292217\n",
      "Epoch 79, loss: 2.289175\n",
      "Epoch 80, loss: 2.287697\n",
      "Epoch 81, loss: 2.286720\n",
      "Epoch 82, loss: 2.294197\n",
      "Epoch 83, loss: 2.289815\n",
      "Epoch 84, loss: 2.289348\n",
      "Epoch 85, loss: 2.284957\n",
      "Epoch 86, loss: 2.288867\n",
      "Epoch 87, loss: 2.291891\n",
      "Epoch 88, loss: 2.289216\n",
      "Epoch 89, loss: 2.288442\n",
      "Epoch 90, loss: 2.287431\n",
      "Epoch 91, loss: 2.287665\n",
      "Epoch 92, loss: 2.290194\n",
      "Epoch 93, loss: 2.283558\n",
      "Epoch 94, loss: 2.292710\n",
      "Epoch 95, loss: 2.284917\n",
      "Epoch 96, loss: 2.289439\n",
      "Epoch 97, loss: 2.281841\n",
      "Epoch 98, loss: 2.282656\n",
      "Epoch 99, loss: 2.295701\n",
      "Epoch 100, loss: 2.288346\n",
      "Epoch 101, loss: 2.282312\n",
      "Epoch 102, loss: 2.293159\n",
      "Epoch 103, loss: 2.287341\n",
      "Epoch 104, loss: 2.295109\n",
      "Epoch 105, loss: 2.290705\n",
      "Epoch 106, loss: 2.287597\n",
      "Epoch 107, loss: 2.293335\n",
      "Epoch 108, loss: 2.286988\n",
      "Epoch 109, loss: 2.289824\n",
      "Epoch 110, loss: 2.284612\n",
      "Epoch 111, loss: 2.288602\n",
      "Epoch 112, loss: 2.284262\n",
      "Epoch 113, loss: 2.276635\n",
      "Epoch 114, loss: 2.284465\n",
      "Epoch 115, loss: 2.290235\n",
      "Epoch 116, loss: 2.284428\n",
      "Epoch 117, loss: 2.283072\n",
      "Epoch 118, loss: 2.284567\n",
      "Epoch 119, loss: 2.288360\n",
      "Epoch 120, loss: 2.287373\n",
      "Epoch 121, loss: 2.281433\n",
      "Epoch 122, loss: 2.285177\n",
      "Epoch 123, loss: 2.286525\n",
      "Epoch 124, loss: 2.291101\n",
      "Epoch 125, loss: 2.287309\n",
      "Epoch 126, loss: 2.289181\n",
      "Epoch 127, loss: 2.284342\n",
      "Epoch 128, loss: 2.279468\n",
      "Epoch 129, loss: 2.281675\n",
      "Epoch 130, loss: 2.280702\n",
      "Epoch 131, loss: 2.282151\n",
      "Epoch 132, loss: 2.283275\n",
      "Epoch 133, loss: 2.284730\n",
      "Epoch 134, loss: 2.283483\n",
      "Epoch 135, loss: 2.290552\n",
      "Epoch 136, loss: 2.287187\n",
      "Epoch 137, loss: 2.282528\n",
      "Epoch 138, loss: 2.285276\n",
      "Epoch 139, loss: 2.285911\n",
      "Epoch 140, loss: 2.287108\n",
      "Epoch 141, loss: 2.290941\n",
      "Epoch 142, loss: 2.284134\n",
      "Epoch 143, loss: 2.283310\n",
      "Epoch 144, loss: 2.282954\n",
      "Epoch 145, loss: 2.276014\n",
      "Epoch 146, loss: 2.277979\n",
      "Epoch 147, loss: 2.282124\n",
      "Epoch 148, loss: 2.286050\n",
      "Epoch 149, loss: 2.278591\n",
      "Epoch 150, loss: 2.286306\n",
      "Epoch 151, loss: 2.281964\n",
      "Epoch 152, loss: 2.281145\n",
      "Epoch 153, loss: 2.280907\n",
      "Epoch 154, loss: 2.280861\n",
      "Epoch 155, loss: 2.282986\n",
      "Epoch 156, loss: 2.273989\n",
      "Epoch 157, loss: 2.284389\n",
      "Epoch 158, loss: 2.278966\n",
      "Epoch 159, loss: 2.281164\n",
      "Epoch 160, loss: 2.281347\n",
      "Epoch 161, loss: 2.272623\n",
      "Epoch 162, loss: 2.278436\n",
      "Epoch 163, loss: 2.281840\n",
      "Epoch 164, loss: 2.278808\n",
      "Epoch 165, loss: 2.274816\n",
      "Epoch 166, loss: 2.279737\n",
      "Epoch 167, loss: 2.278738\n",
      "Epoch 168, loss: 2.279561\n",
      "Epoch 169, loss: 2.281477\n",
      "Epoch 170, loss: 2.277604\n",
      "Epoch 171, loss: 2.282570\n",
      "Epoch 172, loss: 2.269595\n",
      "Epoch 173, loss: 2.278381\n",
      "Epoch 174, loss: 2.285878\n",
      "Epoch 175, loss: 2.279298\n",
      "Epoch 176, loss: 2.272816\n",
      "Epoch 177, loss: 2.276793\n",
      "Epoch 178, loss: 2.280299\n",
      "Epoch 179, loss: 2.275961\n",
      "Epoch 180, loss: 2.275690\n",
      "Epoch 181, loss: 2.277486\n",
      "Epoch 182, loss: 2.278243\n",
      "Epoch 183, loss: 2.280883\n",
      "Epoch 184, loss: 2.277883\n",
      "Epoch 185, loss: 2.277335\n",
      "Epoch 186, loss: 2.278786\n",
      "Epoch 187, loss: 2.275038\n",
      "Epoch 188, loss: 2.276584\n",
      "Epoch 189, loss: 2.274136\n",
      "Epoch 190, loss: 2.279420\n",
      "Epoch 191, loss: 2.288977\n",
      "Epoch 192, loss: 2.273733\n",
      "Epoch 193, loss: 2.282091\n",
      "Epoch 194, loss: 2.270576\n",
      "Epoch 195, loss: 2.272541\n",
      "Epoch 196, loss: 2.270721\n",
      "Epoch 197, loss: 2.275973\n",
      "Epoch 198, loss: 2.276938\n",
      "Epoch 199, loss: 2.273464\n",
      "Epoch 0, loss: 2.301951\n",
      "Epoch 1, loss: 2.302351\n",
      "Epoch 2, loss: 2.302370\n",
      "Epoch 3, loss: 2.302607\n",
      "Epoch 4, loss: 2.301334\n",
      "Epoch 5, loss: 2.302517\n",
      "Epoch 6, loss: 2.301176\n",
      "Epoch 7, loss: 2.303442\n",
      "Epoch 8, loss: 2.301956\n",
      "Epoch 9, loss: 2.300761\n",
      "Epoch 10, loss: 2.301564\n",
      "Epoch 11, loss: 2.301902\n",
      "Epoch 12, loss: 2.301336\n",
      "Epoch 13, loss: 2.301647\n",
      "Epoch 14, loss: 2.300934\n",
      "Epoch 15, loss: 2.301298\n",
      "Epoch 16, loss: 2.300162\n",
      "Epoch 17, loss: 2.301609\n",
      "Epoch 18, loss: 2.298628\n",
      "Epoch 19, loss: 2.301680\n",
      "Epoch 20, loss: 2.298262\n",
      "Epoch 21, loss: 2.300252\n",
      "Epoch 22, loss: 2.298194\n",
      "Epoch 23, loss: 2.299595\n",
      "Epoch 24, loss: 2.296881\n",
      "Epoch 25, loss: 2.297638\n",
      "Epoch 26, loss: 2.301604\n",
      "Epoch 27, loss: 2.298855\n",
      "Epoch 28, loss: 2.298059\n",
      "Epoch 29, loss: 2.299109\n",
      "Epoch 30, loss: 2.297658\n",
      "Epoch 31, loss: 2.299479\n",
      "Epoch 32, loss: 2.302011\n",
      "Epoch 33, loss: 2.298353\n",
      "Epoch 34, loss: 2.297918\n",
      "Epoch 35, loss: 2.296534\n",
      "Epoch 36, loss: 2.296523\n",
      "Epoch 37, loss: 2.295697\n",
      "Epoch 38, loss: 2.296085\n",
      "Epoch 39, loss: 2.297294\n",
      "Epoch 40, loss: 2.295013\n",
      "Epoch 41, loss: 2.292989\n",
      "Epoch 42, loss: 2.294384\n",
      "Epoch 43, loss: 2.295650\n",
      "Epoch 44, loss: 2.296674\n",
      "Epoch 45, loss: 2.297939\n",
      "Epoch 46, loss: 2.294806\n",
      "Epoch 47, loss: 2.295397\n",
      "Epoch 48, loss: 2.298680\n",
      "Epoch 49, loss: 2.292323\n",
      "Epoch 50, loss: 2.293715\n",
      "Epoch 51, loss: 2.293709\n",
      "Epoch 52, loss: 2.295046\n",
      "Epoch 53, loss: 2.290941\n",
      "Epoch 54, loss: 2.295045\n",
      "Epoch 55, loss: 2.292242\n",
      "Epoch 56, loss: 2.298612\n",
      "Epoch 57, loss: 2.291487\n",
      "Epoch 58, loss: 2.290306\n",
      "Epoch 59, loss: 2.293396\n",
      "Epoch 60, loss: 2.294718\n",
      "Epoch 61, loss: 2.292758\n",
      "Epoch 62, loss: 2.297690\n",
      "Epoch 63, loss: 2.291233\n",
      "Epoch 64, loss: 2.293206\n",
      "Epoch 65, loss: 2.292312\n",
      "Epoch 66, loss: 2.295963\n",
      "Epoch 67, loss: 2.292525\n",
      "Epoch 68, loss: 2.291997\n",
      "Epoch 69, loss: 2.289627\n",
      "Epoch 70, loss: 2.292501\n",
      "Epoch 71, loss: 2.292663\n",
      "Epoch 72, loss: 2.292811\n",
      "Epoch 73, loss: 2.292829\n",
      "Epoch 74, loss: 2.290155\n",
      "Epoch 75, loss: 2.290396\n",
      "Epoch 76, loss: 2.290398\n",
      "Epoch 77, loss: 2.295756\n",
      "Epoch 78, loss: 2.287479\n",
      "Epoch 79, loss: 2.293902\n",
      "Epoch 80, loss: 2.286596\n",
      "Epoch 81, loss: 2.286386\n",
      "Epoch 82, loss: 2.288925\n",
      "Epoch 83, loss: 2.289474\n",
      "Epoch 84, loss: 2.288326\n",
      "Epoch 85, loss: 2.295964\n",
      "Epoch 86, loss: 2.292583\n",
      "Epoch 87, loss: 2.292019\n",
      "Epoch 88, loss: 2.285635\n",
      "Epoch 89, loss: 2.285173\n",
      "Epoch 90, loss: 2.291293\n",
      "Epoch 91, loss: 2.286565\n",
      "Epoch 92, loss: 2.289152\n",
      "Epoch 93, loss: 2.281064\n",
      "Epoch 94, loss: 2.287604\n",
      "Epoch 95, loss: 2.294102\n",
      "Epoch 96, loss: 2.287650\n",
      "Epoch 97, loss: 2.285135\n",
      "Epoch 98, loss: 2.291793\n",
      "Epoch 99, loss: 2.285331\n",
      "Epoch 100, loss: 2.287060\n",
      "Epoch 101, loss: 2.289440\n",
      "Epoch 102, loss: 2.282883\n",
      "Epoch 103, loss: 2.287317\n",
      "Epoch 104, loss: 2.286569\n",
      "Epoch 105, loss: 2.293522\n",
      "Epoch 106, loss: 2.288233\n",
      "Epoch 107, loss: 2.291991\n",
      "Epoch 108, loss: 2.287198\n",
      "Epoch 109, loss: 2.288514\n",
      "Epoch 110, loss: 2.282424\n",
      "Epoch 111, loss: 2.286759\n",
      "Epoch 112, loss: 2.283715\n",
      "Epoch 113, loss: 2.290048\n",
      "Epoch 114, loss: 2.282905\n",
      "Epoch 115, loss: 2.285902\n",
      "Epoch 116, loss: 2.286132\n",
      "Epoch 117, loss: 2.280319\n",
      "Epoch 118, loss: 2.287444\n",
      "Epoch 119, loss: 2.284024\n",
      "Epoch 120, loss: 2.287795\n",
      "Epoch 121, loss: 2.278386\n",
      "Epoch 122, loss: 2.285471\n",
      "Epoch 123, loss: 2.286749\n",
      "Epoch 124, loss: 2.284965\n",
      "Epoch 125, loss: 2.283176\n",
      "Epoch 126, loss: 2.292333\n",
      "Epoch 127, loss: 2.280475\n",
      "Epoch 128, loss: 2.282239\n",
      "Epoch 129, loss: 2.284031\n",
      "Epoch 130, loss: 2.279226\n",
      "Epoch 131, loss: 2.278076\n",
      "Epoch 132, loss: 2.282050\n",
      "Epoch 133, loss: 2.284236\n",
      "Epoch 134, loss: 2.284930\n",
      "Epoch 135, loss: 2.282253\n",
      "Epoch 136, loss: 2.284209\n",
      "Epoch 137, loss: 2.291814\n",
      "Epoch 138, loss: 2.286979\n",
      "Epoch 139, loss: 2.280423\n",
      "Epoch 140, loss: 2.284803\n",
      "Epoch 141, loss: 2.279429\n",
      "Epoch 142, loss: 2.279934\n",
      "Epoch 143, loss: 2.287329\n",
      "Epoch 144, loss: 2.281880\n",
      "Epoch 145, loss: 2.284832\n",
      "Epoch 146, loss: 2.286227\n",
      "Epoch 147, loss: 2.277120\n",
      "Epoch 148, loss: 2.272827\n",
      "Epoch 149, loss: 2.279352\n",
      "Epoch 150, loss: 2.291409\n",
      "Epoch 151, loss: 2.291680\n",
      "Epoch 152, loss: 2.279919\n",
      "Epoch 153, loss: 2.279093\n",
      "Epoch 154, loss: 2.273966\n",
      "Epoch 155, loss: 2.276288\n",
      "Epoch 156, loss: 2.292353\n",
      "Epoch 157, loss: 2.278539\n",
      "Epoch 158, loss: 2.281830\n",
      "Epoch 159, loss: 2.285061\n",
      "Epoch 160, loss: 2.281154\n",
      "Epoch 161, loss: 2.276768\n",
      "Epoch 162, loss: 2.280545\n",
      "Epoch 163, loss: 2.274115\n",
      "Epoch 164, loss: 2.276677\n",
      "Epoch 165, loss: 2.278494\n",
      "Epoch 166, loss: 2.268791\n",
      "Epoch 167, loss: 2.282227\n",
      "Epoch 168, loss: 2.282619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, loss: 2.272667\n",
      "Epoch 170, loss: 2.278509\n",
      "Epoch 171, loss: 2.277616\n",
      "Epoch 172, loss: 2.277254\n",
      "Epoch 173, loss: 2.278971\n",
      "Epoch 174, loss: 2.277699\n",
      "Epoch 175, loss: 2.280970\n",
      "Epoch 176, loss: 2.287619\n",
      "Epoch 177, loss: 2.279646\n",
      "Epoch 178, loss: 2.275322\n",
      "Epoch 179, loss: 2.281749\n",
      "Epoch 180, loss: 2.270249\n",
      "Epoch 181, loss: 2.276641\n",
      "Epoch 182, loss: 2.270393\n",
      "Epoch 183, loss: 2.280985\n",
      "Epoch 184, loss: 2.280417\n",
      "Epoch 185, loss: 2.273978\n",
      "Epoch 186, loss: 2.278810\n",
      "Epoch 187, loss: 2.280470\n",
      "Epoch 188, loss: 2.274053\n",
      "Epoch 189, loss: 2.283462\n",
      "Epoch 190, loss: 2.281732\n",
      "Epoch 191, loss: 2.283446\n",
      "Epoch 192, loss: 2.274455\n",
      "Epoch 193, loss: 2.279599\n",
      "Epoch 194, loss: 2.272185\n",
      "Epoch 195, loss: 2.277196\n",
      "Epoch 196, loss: 2.271799\n",
      "Epoch 197, loss: 2.280457\n",
      "Epoch 198, loss: 2.278982\n",
      "Epoch 199, loss: 2.279783\n",
      "lr 1.000000e-05 reg 1.000000e-06 train accuracy: 0.108333 val accuracy: 0.097000\n",
      "lr 1.000000e-05 reg 1.000000e-05 train accuracy: 0.087444 val accuracy: 0.090000\n",
      "lr 1.000000e-05 reg 1.000000e-04 train accuracy: 0.096222 val accuracy: 0.080000\n",
      "lr 1.000000e-04 reg 1.000000e-06 train accuracy: 0.100444 val accuracy: 0.103000\n",
      "lr 1.000000e-04 reg 1.000000e-05 train accuracy: 0.106222 val accuracy: 0.106000\n",
      "lr 1.000000e-04 reg 1.000000e-04 train accuracy: 0.115889 val accuracy: 0.090000\n",
      "lr 1.000000e-03 reg 1.000000e-06 train accuracy: 0.175778 val accuracy: 0.148000\n",
      "lr 1.000000e-03 reg 1.000000e-05 train accuracy: 0.163778 val accuracy: 0.129000\n",
      "lr 1.000000e-03 reg 1.000000e-04 train accuracy: 0.164444 val accuracy: 0.134000\n",
      "lr 5.250000e-03 reg 1.000000e-06 train accuracy: 0.217000 val accuracy: 0.201000\n",
      "lr 5.250000e-03 reg 1.000000e-05 train accuracy: 0.210778 val accuracy: 0.193000\n",
      "lr 5.250000e-03 reg 1.000000e-04 train accuracy: 0.211444 val accuracy: 0.188000\n",
      "lr 6.270785e-02 reg 1.000000e-06 train accuracy: 0.243000 val accuracy: 0.234000\n",
      "lr 6.270785e-02 reg 1.000000e-05 train accuracy: 0.241556 val accuracy: 0.230000\n",
      "lr 6.270785e-02 reg 1.000000e-04 train accuracy: 0.240889 val accuracy: 0.226000\n",
      "best validation accuracy achieved during cross-validation: 0.234000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "learning_rates = [1e-3, 1e-4, 1e-5, 10 ** np.random.uniform(-6, 1), 5.25E-03]\n",
    "regularization_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "grid_search = [ (lr, rg) for lr in learning_rates for rg in regularization_strengths]\n",
    "\n",
    "for lr, rg in grid_search:\n",
    "    # Create a new Softmax instance\n",
    "    softmax_model = linear_classifer.LinearSoftmaxClassifier()\n",
    "    # Train the model with current parameters\n",
    "    softmax_model.fit(train_X, train_y,batch_size=batch_size, learning_rate=lr, reg=rg, epochs=num_epochs)\n",
    "    # Predict values for training set\n",
    "    y_train_pred = softmax_model.predict(train_X)\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = multiclass_accuracy(y_train_pred,train_y)\n",
    "    # Predict values for validation set\n",
    "    y_val_pred = softmax_model.predict(val_X)\n",
    "    # Calculate accuracy\n",
    "    val_accuracy = multiclass_accuracy(y_val_pred, val_y)\n",
    "    # Save results\n",
    "    results[(lr,rg)] = (train_accuracy, val_accuracy)\n",
    "    if best_val < val_accuracy:\n",
    "        best_val = val_accuracy\n",
    "        best_classifier = softmax_model\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.190000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
